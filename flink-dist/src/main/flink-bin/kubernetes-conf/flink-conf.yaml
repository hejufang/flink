################################################################################
#  Licensed to the Apache Software Foundation (ASF) under one
#  or more contributor license agreements.  See the NOTICE file
#  distributed with this work for additional information
#  regarding copyright ownership.  The ASF licenses this file
#  to you under the Apache License, Version 2.0 (the
#  "License"); you may not use this file except in compliance
#  with the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
# limitations under the License.
################################################################################

# shell configurations which must be set at top level.

common:
  parallelism.default: 1
  stream-partitioner.default: rescale
  #  io.tmp.dirs: /opt/tmp/flink
  #  taskmanager.tmp.dirs: /opt/tmp/flink
  state.backend: rocksdb
  state.checkpoints.dir: "file:///opt/tiger/flink_deploy/logs/"
  #  state.backend.rocksdb.localdir: /opt/tmp/flink-dev/flink-rocksdb-local
  taskmanager.memory.network.fraction: 0.3

  # kubernetes configuration
  kubernetes.flink.log.dir: /var/log/tiger
  kubernetes.flink.conf.dir: /opt/tiger/flink_deploy/conf
  kubernetes.entry.path: /opt/tiger/flink_deploy/bin/kubernetes-entry.sh
  kubernetes.container-start-command-template: "%java% %classpath% %jvmmem% %jvmopts% %logging% %class% %args%"
  kubernetes.namespace: flink
  kubernetes.taskmanager.cpu: 2.0
  kubernetes.jobmanager.cpu: 1.0
  kubernetes.jobmanager.service-account: flink
  kubernetes.deployment.annotations: "pod.tce.kubernetes.io/mountHostPath:true"
  kubernetes.container.work.dir: /opt/tiger/workdir
  pipeline.file-mounted-path: /opt/tiger/workdir

  prune.buffer.threshold: 262144

  historyserver.web.tmpdir: /opt/tmp/flink/historyserver/tmpdir

  akka.ask.timeout: 60 s

  flink.lib.path: lib_new_conf

  restart-strategy: failure-rate
  restart-strategy.failure-rate.max-failures-per-interval: 5
  restart-strategy.failure-rate.failure-rate-interval: 30 min
  restart-strategy.failure-rate.delay: 20 s

  flink.partition-discovery.interval-millis: 600000 # default 10min for kafka partition auto discovery interval.

  # restart-strategy: fixed-delay
  # restart-strategy.fixed-delay.attempts: 100
  # restart-strategy.fixed-delay.delay: 20 s
  # The failover strategy, i.e., how the job computation recovers from task failures.
  # Only restart tasks that may have been affected by the task failure, which typically includes
  # downstream tasks and potentially upstream tasks if their produced data is no longer available for consumption.

  # TODO(zoudan):add region restart monitor
  jobmanager.execution.failover-strategy: region

  # Table & SQL configurations.
  # validate before execute, e.g. hive permission check.
  table.exec.validate-before-execute: true

  #==============================================================================
  # Rest & web frontend
  #==============================================================================

  nmclientasync.enabled: true
  taskmanager.initial-on-start: false
  resourcemanager.taskmanager-timeout: 600000
  slot.request.timeout: 300000

  # client configs
  rest.await-leader-timeout: 300000
  rest.retry.max-attempts: 30
  rest.retry.delay: 10000

  # GC log options
  flink.gc.log.opts: -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=100M
  flink.gc.g1: false
  flink.parallel.gc.thread.use.cores: true

  jobmanager.upload-user-jar: false

  web.submit.enable: false
  resourcemanager.shuffle-pending-slots: false

  # The Netty send and receive buffer size.
  taskmanager.network.netty.sendReceiveBufferSize: 4194304

  taskmanager.number-extra-initial: 0
  taskmanager.extra-initial-fraction: 0

  jobmanager.execution.schedule-task-fairly: false

  # External jars we needed, use ',' to separator different jars.
  flink.external.jar.dependencies: "connectors/flink-connector-bmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-databus-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-doris-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-htap_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-jdbc_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-loghouse-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-metrics-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-redis-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-rocketmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-tos-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-hive-1.2.2-bd31_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-kafka-0.10_2.11-1.11-byted-SNAPSHOT.jar,formats/flink-binlog-1.11-byted-SNAPSHOT.jar,formats/flink-bytes-1.11-byted-SNAPSHOT.jar,formats/flink-json-1.11-byted-SNAPSHOT.jar,formats/flink-pb-1.11-byted-SNAPSHOT.jar,formats/flink-sequence-file-1.11-byted-SNAPSHOT.jar"

  job.work.dir: ${hdfs.prefix}/${clusterName}/1.11/
  pipeline.download-template: "/opt/tiger/flink_deploy/bin/flink download -src '%files%' -dest %target% > /var/log/tiger/flink-download.log 2>&1"

cloudnative-hl:
  dc: cn
  clusterName: cloudnative-hl
  high-availability.zookeeper.quorum: 10.226.22.110:2181,10.226.22.38:2181,10.226.22.84:2181,10.226.22.85:2181,10.226.22.90:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220
  # service & ingress related
  kubernetes.rest-service.exposed.type: ClusterIP
  kubernetes.ingress.annotations: "nginx.ingress.kubernetes.io/proxy-body-size:100m,nginx.ingress.kubernetes.io/proxy-connect-timeout:300,nginx.ingress.kubernetes.io/proxy-send-timeout:300,nginx.ingress.kubernetes.io/proxy-read-timeout:300"
  kubernetes.ingress.enable: true
  kubernetes.ingress.host: "hl-cloudnative.byted.org"

cloudnative-lf:
  dc: cn
  clusterName: cloudnative-lf
  high-availability.zookeeper.quorum: 10.226.22.110:2181,10.226.22.38:2181,10.226.22.84:2181,10.226.22.85:2181,10.226.22.90:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220
  # service & ingress related
  kubernetes.rest-service.exposed.type: ClusterIP
  kubernetes.ingress.annotations: "nginx.ingress.kubernetes.io/proxy-body-size:100m,nginx.ingress.kubernetes.io/proxy-connect-timeout:300,nginx.ingress.kubernetes.io/proxy-send-timeout:300,nginx.ingress.kubernetes.io/proxy-read-timeout:300"
  kubernetes.ingress.enable: true
  kubernetes.ingress.host: "lf-cloudnative.byted.org"

cloudnative-lq:
  dc: cn
  clusterName: cloudnative-lq
  high-availability.zookeeper.quorum: 10.226.22.110:2181,10.226.22.38:2181,10.226.22.84:2181,10.226.22.85:2181,10.226.22.90:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220
  # service & ingress related
  kubernetes.rest-service.exposed.type: ClusterIP
  kubernetes.ingress.annotations: "nginx.ingress.kubernetes.io/proxy-body-size:100m,nginx.ingress.kubernetes.io/proxy-connect-timeout:300,nginx.ingress.kubernetes.io/proxy-send-timeout:300,nginx.ingress.kubernetes.io/proxy-read-timeout:300"
  kubernetes.ingress.enable: true
  kubernetes.ingress.host: "lq-cloudnative.byted.org"

edge:
  dc: cn
  clusterName: edge
  state.checkpoints.dir: "file:///opt/tiger/flink_deploy/logs/"

# Add those items because flink needs to have different hdfs prefix for different execution environment.
flink:
  dc: cn
  clusterName: flink
  high-availability.zookeeper.quorum: 10.17.58.36:2181,10.17.58.40:2181,10.17.58.44:2181,10.17.58.45:2181,10.17.58.78:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  checkpoint.hdfs.prefix: hdfs://hdfsvip/home/byte_flink_checkpoint_20210220
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics

dw:
  dc: cn
  clusterName: dw
  high-availability.zookeeper.quorum: 10.11.43.39:2184,10.11.43.66:2184,10.224.152.92:2184,10.224.71.64:2184,10.224.71.66:2184
  hdfs.prefix: hdfs://haruna/flink_lf
  checkpoint.hdfs.prefix: hdfs://hdfsvip/home/byte_flink_checkpoint_20210220
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics

lepad:
  dc: cn
  clusterName: lepad
  high-availability.zookeeper.quorum: 10.8.32.25:2184,10.8.32.68:2184,10.8.32.74:2184,10.8.32.72:2184,10.8.39.142:2184
  hdfs.prefix: hdfs://haruna/flink_lf
  checkpoint.hdfs.prefix: hdfs://hdfsvip/home/byte_flink_checkpoint_20210220
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics
  bytedance.streaming.yarn.check.application.name.unique.region: true
  taskmanager.network.netty.client.tcp-user-timeout-seconds: 600

larva:
  dc: cn
  clusterName: larva
  high-availability.zookeeper.quorum: 10.8.32.25:2184,10.8.32.68:2184,10.8.32.74:2184,10.8.32.72:2184,10.8.39.142:2184
  hdfs.prefix: hdfs://haruna/flink_lf
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220
  taskmanager.network.netty.client.tcp-user-timeout-seconds: 600

locst:
  dc: cn
  clusterName: locst
  high-availability.zookeeper.quorum: 10.224.193.108:2181,10.224.193.109:2181,10.224.193.93:2181,10.224.193.95:2181,10.224.193.96:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220

oryx:
  dc: cn
  clusterName: oryx
  high-availability.zookeeper.quorum: 10.8.32.25:2184,10.8.32.68:2184,10.8.32.74:2184,10.8.32.72:2184,10.8.39.142:2184
  hdfs.prefix: hdfs://haruna/flink_lf
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220

default:
  dc: cn
  clusterName: default
  high-availability.zookeeper.quorum: 10.8.32.25:2184,10.8.32.68:2184,10.8.32.74:2184,10.8.32.72:2184,10.8.39.142:2184
  hdfs.prefix: hdfs://haruna/flink_lf
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220

leser:
  dc: cn
  clusterName: leser
  high-availability.zookeeper.quorum: 10.8.32.25:2184,10.8.32.68:2184,10.8.32.74:2184,10.8.32.72:2184,10.8.39.142:2184
  hdfs.prefix: hdfs://haruna/flink_lf
  checkpoint.hdfs.prefix: hdfs://hdfsvip/home/byte_flink_checkpoint_20210220
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics
  yarn.provided.lib.dirs.enabled: true

lobst:
  dc: cn
  clusterName: lobst
  high-availability.zookeeper.quorum: 10.8.32.25:2184,10.8.32.68:2184,10.8.32.74:2184,10.8.32.72:2184,10.8.39.142:2184
  hdfs.prefix: hdfs://haruna/flink_lf
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220

wj:
  dc: cn
  clusterName: wj
  high-availability.zookeeper.quorum: 10.8.32.25:2184,10.8.32.68:2184,10.8.32.74:2184,10.8.32.72:2184,10.8.39.142:2184
  hdfs.prefix: hdfs://haruna/flink_hl
  checkpoint.hdfs.prefix: hdfs://haruna/flink_hl

hyrax:
  dc: cn
  clusterName: hyrax
  high-availability.zookeeper.quorum: 10.23.72.70:2181,10.23.73.159:2181,10.23.73.222:2181,10.23.73.69:2181,10.23.73.81:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  checkpoint.hdfs.prefix: hdfs://hdfsvip/home/byte_flink_checkpoint_20210220
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics

horse:
  dc: cn
  clusterName: horse
  high-availability.zookeeper.quorum: 10.226.22.110:2181,10.226.22.38:2181,10.226.22.84:2181,10.226.22.85:2181,10.226.22.90:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  checkpoint.hdfs.prefix: hdfs://hdfsvip/home/byte_flink_checkpoint_20210220
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics
  bytedance.streaming.yarn.check.application.name.unique.region: true
  taskmanager.network.netty.client.tcp-user-timeout-seconds: 600

hibis:
  dc: cn
  clusterName: hibis
  high-availability.zookeeper.quorum: 10.226.22.110:2181,10.226.22.38:2181,10.226.22.84:2181,10.226.22.85:2181,10.226.22.90:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220

topi:
  dc: cn
  clusterName: topi
  high-availability.zookeeper.quorum: 10.226.22.110:2181,10.226.22.38:2181,10.226.22.84:2181,10.226.22.85:2181,10.226.22.90:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220

hippo:
  dc: cn
  clusterName: hippo
  high-availability.zookeeper.quorum: 10.226.22.110:2181,10.226.22.38:2181,10.226.22.84:2181,10.226.22.85:2181,10.226.22.90:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220

heron:
  dc: cn
  clusterName: heron
  high-availability.zookeeper.quorum: 10.226.22.110:2181,10.226.22.38:2181,10.226.22.84:2181,10.226.22.85:2181,10.226.22.90:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220

hamer:
  dc: cn
  clusterName: hamer
  high-availability.zookeeper.quorum: 10.226.22.110:2181,10.226.22.38:2181,10.226.22.84:2181,10.226.22.85:2181,10.226.22.90:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220

hyena:
  dc: cn
  clusterName: hyena
  high-availability.zookeeper.quorum: 10.226.22.110:2181,10.226.22.38:2181,10.226.22.84:2181,10.226.22.85:2181,10.226.22.90:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220

hound:
  dc: cn
  clusterName: hound
  high-availability.zookeeper.quorum: 10.226.22.110:2181,10.226.22.38:2181,10.226.22.84:2181,10.226.22.85:2181,10.226.22.90:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220

quail:
  dc: cn
  clusterName: quail
  high-availability.zookeeper.quorum: 10.129.16.103:2181,10.129.19.99:2181,10.129.36.17:2181,10.129.42.156:2181,10.129.42.93:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220

quoll:
  dc: cn
  clusterName: quoll
  high-availability.zookeeper.quorum: 10.129.16.103:2181,10.129.19.99:2181,10.129.36.17:2181,10.129.42.156:2181,10.129.42.93:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220

quoka:
  dc: cn
  clusterName: quoka
  high-availability.zookeeper.quorum: 10.129.16.103:2181,10.129.19.99:2181,10.129.36.17:2181,10.129.42.156:2181,10.129.42.93:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  checkpoint.hdfs.prefix: hdfs://hdfsvip/home/byte_flink_checkpoint_20210220
  cluster.evenly-spread-out-slots: true
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics
  ipv6.enabled: true
  taskmanager.network.netty.client.tcp-user-timeout-seconds: 600

quaga:
  dc: cn
  clusterName: quaga
  high-availability.zookeeper.quorum: 10.227.165.243:2181,10.227.165.222:2181,10.227.178.242:2181,10.227.174.244:2181,10.227.187.245:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220
  bytedance.streaming.yarn.gang-scheduler.container-descheduler.enable: true
  ipv6.enabled: true
  taskmanager.network.netty.client.tcp-user-timeout-seconds: 600

quele:
  dc: cn
  clusterName: quele
  high-availability.zookeeper.quorum: 10.227.165.243:2181,10.227.165.222:2181,10.227.178.242:2181,10.227.174.244:2181,10.227.187.245:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  checkpoint.hdfs.prefix: hdfs://haruna/home/byte_flink_checkpoint_20210220
  bytedance.streaming.yarn.gang-scheduler.container-descheduler.enable: true

camel:
  dc: cn
  clusterName: camel
  high-availability.zookeeper.quorum: 10.148.16.172:2181,10.148.16.37:2181,10.148.16.90:2181,10.148.20.100:2181,10.148.20.98:2181
  hdfs.prefix: hdfs://haruna/flink_cr
  checkpoint.hdfs.prefix: hdfs://haruna/flink_cr

stork:
  dc: sg
  clusterName: stork
  high-availability.zookeeper.quorum: 10.105.4.10:2181,10.105.4.21:2181,10.105.4.35:2181,10.105.4.91:2181,10.105.4.254:2181
  hdfs.prefix: hdfs://harunasgee/flink_sgee
  checkpoint.hdfs.prefix: hdfs://harunasgee/flink_sgee
  dashboard.data_source: bytetsd_sgee
  dtop.data_source: dtop_alisg
  dtop.database: dtop_alisg
  smart-resources.service-name: data.inf.sr_estimater.service.alisg
  kafka_server_url: http://kafka-config-sg.byted.org
  log4j.appender.databus.channel: yarn_container_level_log_sg
  docker.server: image-manager.byted.org
  docker.hub: aliyun-sin-hub.byted.org
  docker.region: Aliyun_SG
  save-meta.enabled: false
  table.exec.hive.permission-check.gemini-server-url: http://gemini-sg.byted.org/api/query/sg/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false

shark:
  dc: sg
  clusterName: shark
  high-availability.zookeeper.quorum: 10.126.35.119:2181,10.126.35.162:2181,10.126.35.163:2181,10.126.35.186:2181,10.126.35.189:2181
  hdfs.prefix: hdfs://harunasglark/flink_sglark
  checkpoint.hdfs.prefix:  hdfs://harunasglark/flink_sglark
  dashboard.data_source: bytetsd_sgsaas1lark
  dtop.data_source: dtop_alisg
  dtop.database: dtop_alisg
  smart-resources.service-name: data.inf.sr_estimater.service.alisg
  kafka_server_url: http://kafka-config-sg.byted.org
  log4j.appender.databus.channel: yarn_container_level_log_sg
  docker.server: image-manager.byted.org
  docker.hub: aliyun-sin-hub.byted.org
  docker.region: Aliyun_SG
  save-meta.enabled: false
  table.exec.hive.permission-check.gemini-server-url: http://gemini-sg.byted.org/api/query/sg/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false

marmt:
  dc: va
  clusterName: marmt
  high-availability.zookeeper.quorum: 10.231.131.120:2181,10.231.131.100:2181,10.231.131.112:2181,10.231.131.124:2181,10.231.131.106:2181
  hdfs.prefix: hdfs://harunavaali/flink_maliva
  checkpoint.hdfs.prefix: hdfs://harunavaali/flink_maliva
  dashboard.data_source: bytetsd_gva
  dtop.data_source: dtop_maliva
  dtop.database: dtop_maliva
  smart-resources.service-name: data.inf.sr_estimater.service.maliva
  kafka_server_url: http://kafka-config-va.byted.org
  log4j.appender.databus.channel: yarn_container_level_log_mva_aliyun
  yaop_url: http://yaop-us.bytedance.net
  docker.server: image-manager.byted.org
  docker.hub: aliyun-va-hub.byted.org
  docker.region: Aliyun_VA
  jobmeta.db.name: flink_meta_va
  grafana.domain_url: "https://grafana-us.byted.org"
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_va
  table.exec.hive.permission-check.gemini-server-url: http://gemini-maliva.byted.org/api/query/i18n/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false

macaw:
  dc: va
  clusterName: macaw
  high-availability.zookeeper.quorum: 10.231.131.120:2181,10.231.131.100:2181,10.231.131.112:2181,10.231.131.124:2181,10.231.131.106:2181
  hdfs.prefix: hdfs://harunavaali/flink_maliva
  checkpoint.hdfs.prefix: hdfs://harunavaali/flink_maliva
  dashboard.data_source: bytetsd_gva
  dtop.data_source: dtop_maliva
  dtop.database: dtop_maliva
  smart-resources.service-name: data.inf.sr_estimater.service.maliva
  kafka_server_url: http://kafka-config-va.byted.org
  log4j.appender.databus.channel: yarn_container_level_log_mva_aliyun
  yaop_url: http://yaop-us.bytedance.net
  docker.server: image-manager.byted.org
  docker.hub: aliyun-va-hub.byted.org
  docker.region: Aliyun_VA
  jobmeta.db.name: flink_meta_va
  grafana.domain_url: "https://grafana-us.byted.org"
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_va
  table.exec.hive.permission-check.gemini-server-url: http://gemini-maliva.byted.org/api/query/i18n/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false

mouse:
  dc: va
  clusterName: mouse
  high-availability.zookeeper.quorum: 10.231.131.120:2181,10.231.131.100:2181,10.231.131.112:2181,10.231.131.124:2181,10.231.131.106:2181
  hdfs.prefix: hdfs://harunavaali/flink_maliva
  checkpoint.hdfs.prefix: hdfs://harunavaali/flink_maliva
  dashboard.data_source: bytetsd_gva
  dtop.data_source: dtop_maliva
  dtop.database: dtop_maliva
  smart-resources.service-name: data.inf.sr_estimater.service.maliva
  kafka_server_url: http://kafka-config-va.byted.org
  log4j.appender.databus.channel: yarn_container_level_log_mva_aliyun
  yaop_url: http://yaop-us.bytedance.net
  docker.server: image-manager.byted.org
  docker.hub: aliyun-va-hub.byted.org
  docker.region: Aliyun_VA
  jobmeta.db.name: flink_meta_va
  grafana.domain_url: "https://grafana-us.byted.org"
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_va
  table.exec.hive.permission-check.gemini-server-url: http://gemini-maliva.byted.org/api/query/i18n/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false

grila:
  dc: i18n_gcp
  clusterName: grila
  high-availability.zookeeper.quorum: 10.99.53.218:2181,10.99.53.219:2181,10.99.53.220:2181,10.99.53.221:2181,10.99.53.222
  fs.defaultFS: hdfs://harunava
  hdfs.prefix: hdfs://harunava/home/byte_compute_i18n_gcp
  checkpoint.hdfs.prefix: hdfs://harunava/home/byte_compute_i18n_gcp
  dashboard.data_source: bytetsd_useastred
  dtop.data_source: dtop_i18n
  dtop.database: dtop_i18n
  smart-resources.service-name: data.inf.sr_estimater.service.useast2a
  kafka_server_url: http://kafka-config-gcp.byted.org
  log4j.appender.databus.channel: yarn_container_level_log_mva_aliyun
  docker.server: image-manager.byted.org
  docker.hub: useast-red-hub.byted.org
  docker.region: US-East-Red
  yaop_url: http://yaop-gcp.bytedance.net
  jobmeta.db.name: flink_meta
  grafana.domain_url: "https://grafana-i18n.byted.org"
  register-dashboard.token: "Bearer eyJrIjoiSGlWUlRpQlZWcDhsdGJ4ZTRWcGtGc1VHcG8xT2h5UkkiLCJuIjoiZmxpbmsiLCJpZCI6MX0="
  table.exec.hive.permission-check.gemini-server-url: http://gemini-gcp.byted.org/api/query/gcp/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false

gavia:
  dc: i18n_gcp
  clusterName: gavia
  high-availability.zookeeper.quorum: 10.99.53.218:2181,10.99.53.219:2181,10.99.53.220:2181,10.99.53.221:2181,10.99.53.222
  fs.defaultFS: hdfs://harunava
  hdfs.prefix: hdfs://harunava/home/byte_compute_i18n_gcp
  checkpoint.hdfs.prefix: hdfs://harunava/home/byte_compute_i18n_gcp
  dashboard.data_source: bytetsd_useastred
  dtop.data_source: dtop_i18n
  dtop.database: dtop_i18n
  smart-resources.service-name: data.inf.sr_estimater.service.useast2a
  kafka_server_url: http://kafka-config-gcp.byted.org
  log4j.appender.databus.channel: yarn_container_level_log_mva_aliyun
  docker.server: image-manager.byted.org
  docker.hub: useast-red-hub.byted.org
  docker.region: US-East-Red
  yaop_url: http://yaop-gcp.bytedance.net
  jobmeta.db.name: flink_meta
  grafana.domain_url: "https://grafana-i18n.byted.org"
  register-dashboard.token: "Bearer eyJrIjoiSGlWUlRpQlZWcDhsdGJ4ZTRWcGtGc1VHcG8xT2h5UkkiLCJuIjoiZmxpbmsiLCJpZCI6MX0="
  table.exec.hive.permission-check.gemini-server-url: http://gemini-gcp.byted.org/api/query/gcp/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false

otter:
  dc: us-ttp
  clusterName: otter
  high-availability.zookeeper.quorum: 10.113.156.88:2187,10.113.156.226:2187,10.113.156.244:2187,10.113.156.4:2187,10.113.156.42:2187
  hdfs.prefix: hdfs://harunaoci/flink_oci
  checkpoint.hdfs.prefix: hdfs://harunaoci/home/byte_flink_checkpoint
  dashboard.data_source: bytetsd_us_ttp
  dtop.data_source: dtop_gcp
  dtop.database: dtop_gcp
  smart-resources.service-name: data.inf.sr_estimater
  kafka_server_url: https://kafka-config-tx.tiktokd.net
  log4j.appender.databus.channel: yarn_container_level_log_mva_aliyun
  docker.server:  image-manager.Byted.org
  docker.hub: hub.tiktokd.org
  docker.region: US-TTP
  yaop_url: http://yaop-tx.tiktokd.org
  jobmeta.db.name: flink_meta_us_ttp
  grafana.domain_url: "https://grafana-tx.tiktokd.org"
  register-dashboard.token: "Bearer eyJrIjoibzBLV1VrdkRIVkt6eW9UQlN1UmNNRXQ2MjZuOTlJTFMiLCJuIjoiYmVhcmVyIiwiaWQiOjF9"
  table.exec.hive.permission-check.gemini-server-url: http://gemini-oci.tiktokd.org/api/query/oci/verifyUsersPrivilege

octop:
  dc: us-ttp
  clusterName: octop
  high-availability.zookeeper.quorum: 10.113.156.88:2187,10.113.156.226:2187,10.113.156.244:2187,10.113.156.4:2187,10.113.156.42:2187
  hdfs.prefix: hdfs://harunaoci/flink_oci
  checkpoint.hdfs.prefix: hdfs://harunaoci/home/byte_flink_checkpoint
  dashboard.data_source: bytetsd_us_ttp
  dtop.data_source: dtop_gcp
  dtop.database: dtop_gcp
  smart-resources.service-name: data.inf.sr_estimater
  kafka_server_url: https://kafka-config-tx.tiktokd.net
  log4j.appender.databus.channel: yarn_container_level_log_mva_aliyun
  docker.server:  image-manager.Byted.org
  docker.hub: hub.tiktokd.org
  docker.region: US-TTP
  yaop_url: http://yaop-tx.tiktokd.org
  jobmeta.db.name: flink_meta_us_ttp
  grafana.domain_url: "https://grafana-tx.tiktokd.org"
  register-dashboard.token: "Bearer eyJrIjoibzBLV1VrdkRIVkt6eW9UQlN1UmNNRXQ2MjZuOTlJTFMiLCJuIjoiYmVhcmVyIiwiaWQiOjF9"
  table.exec.hive.permission-check.gemini-server-url: http://gemini-oci.tiktokd.org/api/query/oci/verifyUsersPrivilege

alisg:
  dc: sg
  clusterName: alisg
  high-availability.zookeeper.quorum: 10.115.61.129:2181,10.115.61.130:2181,10.115.61.131:2181,10.115.61.132:2181,10.115.61.133:2181
  hdfs.prefix: hdfs://harunasg/flink_alisg
  checkpoint.hdfs.prefix: hdfs://harunasg/home/byte_flink_checkpoint_alisg
  dashboard.data_source: bytetsd_alisg
  dtop.data_source: dtop_alisg
  dtop.database: dtop_alisg
  smart-resources.service-name: data.inf.sr_estimater.service.alisg
  kafka_server_url: http://kafka-config-sg.byted.org
  log4j.appender.databus.channel: yarn_container_level_log_sg
  docker.server: 10.8.27.231:8002
  docker.hub: aliyun-sin-hub.byted.org
  docker.region: Aliyun_SG
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_sg
  table.exec.hive.permission-check.gemini-server-url: http://gemini-sg.byted.org/api/query/sg/verifyUsersPrivilege

sloth:
  dc: sg
  clusterName: sloth
  high-availability.zookeeper.quorum: 10.245.24.23:2181,10.245.30.27:2181,10.245.30.47:2181,10.245.9.27:2181,10.245.9.34:2181
  hdfs.prefix: hdfs://harunasg/flink_sg_sg1
  checkpoint.hdfs.prefix: hdfs://harunasg/home/byte_flink_checkpoint_alisg
  dashboard.data_source: bytetsd_alisg
  dtop.data_source: dtop_alisg
  dtop.database: dtop_alisg
  smart-resources.service-name: data.inf.sr_estimater.service.sg1
  kafka_server_url: http://kafka-config-sg.byted.org
  log4j.appender.databus.channel: yarn_container_level_log_sg
  docker.server: image-manager.byted.org
  docker.hub: aliyun-sin-hub.byted.org
  docker.region: Aliyun_SG
  jobmeta.db.name: flink_meta_sg
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_sg
  table.exec.hive.permission-check.gemini-server-url: http://gemini-sg.byted.org/api/query/sg/verifyUsersPrivilege

boe:
  dc: vm
  clusterName: boe
  high-availability.zookeeper.quorum: 10.225.33.2:2181,10.225.28.3:2181,10.225.33.6:2181,10.225.125.22:2181,10.225.125.29:2181
  hdfs.prefix: hdfs://westeros/flink_boe
  checkpoint.hdfs.prefix: hdfs://westeros/flink_boe
  dtop.database: inf_yarn_dtop_boe
  dtop.data_source: dtop_boe
  dashboard.data_source: bytetsd_boe
  kafka_server_url: http://kafka-config-boe.byted.org
  log4j.appender.databus.channel: yarn_container_level_log_boe
  yaop_url: https://yaop-boe.bytedance.net
  yaop_token: 0ed7ea2fce8d4801a9d79adde7a91211
  cluster.evenly-spread-out-slots: true
  grafana.domain_url: "https://grafana-boe.byted.org"
  register-dashboard.token: "Bearer eyJrIjoiTURSV01QeWNDZXJXYUNBdEhkSU94U2tKajU2M1BVM24iLCJuIjoiZmxpbmsiLCJpZCI6MX0="
  table.exec.hive.permission-check.enabled: false
  yarn.provided.lib.dirs.enabled: false
  taskmanager.network.netty.client.tcp-user-timeout-seconds: 600

cof:
  dc: vm
  clusterName: cof
  high-availability.zookeeper.quorum: 10.225.125.22:2181,10.225.125.29:2181,10.225.28.3:2181,10.225.33.2:2181,10.225.33.6:2181
  hdfs.prefix: hdfs://westeros/flink_cof
  checkpoint.hdfs.prefix: hdfs://westeros/flink_cof
  dashboard.data_source: bytetsd_cof
  kafka_server_url: http://kafka-config.byted.org
  log4j.appender.databus.channel: yarn_container_level_log
  yaop_url: https://yaop-boe.bytedance.net
  yaop_token: 0ed7ea2fce8d4801a9d79adde7a91211
  cluster.evenly-spread-out-slots: true
  table.exec.hive.permission-check.enabled: false
  yarn.provided.lib.dirs.enabled: false

swan:
  dc: ka
  clusterName: swan
  high-availability.zookeeper.quorum: 10.230.2.2:2181,10.230.2.10:2181,10.230.2.7:2181
  hdfs.prefix: hdfs://nestbackend/flink_swan
  checkpoint.hdfs.prefix: hdfs://nestbackend/flink_swan
  table.exec.hive.permission-check.enabled: false
  yarn.provided.lib.dirs.enabled: false

koala:
  dc: ka2
  clusterName: koala
  high-availability.zookeeper.quorum: 10.230.9.118:2181,10.230.9.121:2181,10.230.9.102:2181
  hdfs.prefix: hdfs://nestbackend/flink_swan
  checkpoint.hdfs.prefix: hdfs://nestbackend/flink_swan
  table.exec.hive.permission-check.enabled: false
  yarn.provided.lib.dirs.enabled: false

boei18n:
  dc: vm
  clusterName: boei18n
  high-availability.zookeeper.quorum: 10.231.8.12:2181,10.231.8.51:2181,10.231.8.25:2181,10.231.8.21:2181,10.231.8.29:2181
  hdfs.prefix: hdfs://essos/flink_boei18n
  checkpoint.hdfs.prefix: hdfs://essos/flink_boei18n
  dashboard.data_source: bytetsd_boei18n
  kafka_server_url: http://kafka-config-boei18n.byted.org
  log4j.appender.databus.channel: yarn_container_level_log_boei18n
  yaop_url: http://yaop-boei18n.bytedance.net
  yaop_token: 0ed7ea2fce8d4801a9d79adde7a91211
  docker.hub: aliyun-va-hub.byted.org
  docker.namespace: yarn
  docker.region: Aliyun_VA
  grafana.domain_url: "https://grafana-boei18n.byted.org"
  register-dashboard.token: "Bearer eyJrIjoicndEUGRnNHpUZE9Gcm04VE5QSDdDV3JzbG8wWFZYa3IiLCJuIjoiZmxpbmsiLCJpZCI6MX0="
  table.exec.hive.permission-check.enabled: false
  yarn.provided.lib.dirs.enabled: false
  taskmanager.network.netty.client.tcp-user-timeout-seconds: 600
