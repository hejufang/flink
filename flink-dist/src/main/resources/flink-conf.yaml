################################################################################
#  Licensed to the Apache Software Foundation (ASF) under one
#  or more contributor license agreements.  See the NOTICE file
#  distributed with this work for additional information
#  regarding copyright ownership.  The ASF licenses this file
#  to you under the Apache License, Version 2.0 (the
#  "License"); you may not use this file except in compliance
#  with the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
# limitations under the License.
################################################################################

common:
  # Common Setup Options
  ## Fault Tolerance
  restart-strategy: aggregated-failure-rate
  restart-strategy.failure-rate.max-failures-per-interval: 100
  restart-strategy.failure-rate.failure-rate-interval: 40 min
  restart-strategy.failure-rate.delay: 20 s

  restart-strategy.aggregated-failure-rate.max-failures-per-interval: 100
  restart-strategy.aggregated-failure-rate.failure-rate-interval: 40 min
  restart-strategy.aggregated-failure-rate.delay: 20 s

  jobmanager.execution.failover-strategy: region
  jobmanager.execution.status-dutation-ms: 30000

  restart-strategy.recoverable-failure-rate.fallback-global-restart: true
  restart-strategy.recoverable-failure-rate.enable: true

  ## Checkpoints and State Backends
  state.backend: rocksdb
  state.checkpoints.dir: ${checkpoint.hdfs.prefix}/1.11/flink/fs_checkpoint_dir
  state.backend.incremental: true
  state.checkpoints.num-retained: 3
  state.backend.rocksdb.checkpoint.transfer.thread.num: 4
  state.backend.rocksdb.use-fsync: true
  state.backend.rocksdb.log.level: info_level
  state.backend.rocksdb.stats.dump.period.seconds: 600
  state.backend.rocksdb.monit.running.status: true
  state.backend.rocksdb.force-ssd: false
  state.backend.rocksdb.compaction.level.use-dynamic-size: true
  state.backend.state-file-batch.enable: true
  execution.checkpointing.mode: EXACTLY_ONCE
  execution.checkpointing.timeout: 10 min
  execution.checkpointing.ignore-checkpoints-on-checkpoint-disabled: true
  state.savepoint.location-prefix: ${checkpoint.hdfs.prefix}/savepoints
  state.checkpoints.expired.location-prefix: ${checkpoint.hdfs.prefix}/expire
  checkpoint.discard.historical.num: 3
  checkpoint.discard.delay.time: 1800000
  state.backend.rocksdb.restore-with-sstWriter: true
  state.backend.rocksdb.max-disk-size-in-progress: 30gb
  state.backend.rocksdb.max-sst-size-for-sstWriter: 64mb

  savepoint.scheduler.default.interval: 21600000
  state.backend.rocksdb.memory.managed: false

  ## High Availability
  high-availability: zookeeper
  high-availability.storageDir: ${hdfs.prefix}/${clusterName}/1.11/ha/
  high-availability.complete-checkpoint.storageDir: ${checkpoint.hdfs.prefix}/1.11/flink/ha/
  high-availability.zookeeper.path.root: /${dc}/${clusterName}/flink
  high-availability.zookeeper.client.max-retry-attempts: 20 # default 3
  high-availability.zookeeper.client.connection-timeout: 60000 # default 15000ms
  high-availability.zookeeper.client.retry-wait: 20000 # default 5000ms
  high-availability.zookeeper.client.session-timeout: 180000 # default 60000ms, be careful to change this.

  ## Heartbeat Services
  heartbeat.interval: 40000 # default 10000 ms
  heartbeat.timeout: 180000 # default 50000 ms

  ## Memory Configuration
  taskmanager.memory.managed.fraction: 0.25
  taskmanager.memory.network.fraction: 0.3
  taskmanager.memory.network.max: 2147483648

  ## StreamPipelineOptions
  pipeline.time-characteristic: EventTime

  # Security
  ## Auth with External Systems
  zookeeper.sasl.disable: true

  # Resource Orchestration Frameworks
  ## Yarn
  yarn.application-attempts: 5
  yarn.application-attempt-failures-validity-interval: 3600000
  yarn.per-job-cluster.include-user-jar: FIRST
  yarn.taskmanager.set_token: false
  yarn.conf.cluster_queue_name.enable: true
  yarn.check.application.name.unique: false
  yarn.check.application.name.unique.region: false
  yarn.runtime-conf.qos-level: share
  yarn.res-lake.enabled: false
  flink.jobmanager.yarn.config.yarn.client.failover-max-attempts: -1
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 60000
  flink.jobmanager.yarn.config.yarn.resourcemanager.connect.retry-interval.ms: 1000
  flink.jobmanager.yarn.config.yarn.resourcemanager.connect.max-wait.ms: -1
  flink.client.yarn.config.yarn.client.failover-max-attempts: -1
  flink.client.yarn.config.yarn.resourcemanager.connect.retry-interval.ms: 1000
  flink.client.yarn.config.yarn.resourcemanager.connect.max-wait.ms: 900000
  yarn.provided.lib.dirs.enabled: true

  # kubernetes configuration
  kubernetes.flink.log.dir: /var/log/tiger
  kubernetes.flink.conf.dir: /opt/tiger/flink_deploy/conf
  kubernetes.entry.path: /opt/tiger/flink_deploy/bin/kubernetes-entry.sh
  kubernetes.namespace: flink
  kubernetes.jobmanager.service-account: flink
  kubernetes.deployment.annotations.pod.tce.kubernetes.io/mountHostPath: "true"
  kubernetes.container.work.dir: /opt/tiger/workdir
  kubernetes.web-shell.enabled: true
  kubernetes.stream-log.enabled: true
  kubernetes.stream-log.domain: "cloud.bytedance.net"
  kubernetes.rest-service.exposed.type: ClusterIP
  kubernetes.ingress.annotations.nginx.ingress.kubernetes.io/proxy-body-size: "100m"
  kubernetes.ingress.annotations.nginx.ingress.kubernetes.io/proxy-connect-timeout: "300"
  kubernetes.ingress.annotations.nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
  kubernetes.ingress.annotations.nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
  kubernetes.ingress.enable: true
  kubernetes.podgroup.enable: true
  kubernetes.hostnetwork.enabled: true
  containerized.master.env.KUBERNETES_REQUEST_TIMEOUT: 30000

  pipeline.file-mounted-path: /opt/tiger/workdir

  # Metrics
  metrics.reporters: opentsdb_reporter,databus_reporter
  metrics.reporter.opentsdb_reporter.class: org.apache.flink.metrics.opentsdb.OpentsdbReporter
  metrics.reporter.opentsdb_reporter.interval: 20 SECONDS
  metrics.reporter.databus_reporter.class: org.apache.flink.metrics.databus.DatabusReporter
  metrics.reporter.databus_reporter.interval: 60 SECONDS

  # Grafana
  save-meta.enabled: true
  register-dashboard.enabled: false
  grafana.domain_url: "https://grafana.byted.org"
  register-dashboard.token: "Bearer eyJrIjoiYjZMS0hPSXZybVpOOWJMS3pLRHkwaXRoWWI2RW1UT2oiLCJuIjoianN0b3JtIiwiaWQiOjF9"

  # REST endpoint and Client
  rest.await-leader-timeout: 300000
  rest.retry.max-attempts: 30
  rest.retry.delay: 10000

  # Web UI
  web.submit.enable: false

  # Full JobManager Options
  resourcemanager.taskmanager-timeout: 600000
  slot.request.timeout: 300000
  execution.cancellation.timeout.ms: 300000
  execution.cancellation.timeout.enable: true

  # Full TaskManagerOptions
  taskmanager.network.netty.sendReceiveBufferSize: 4194304
  taskmanager.serializer.prune.buffer.threshold: 262144
  taskmanager.network.netty.connection.tcp-user-timeout-seconds: 600

  # RPC / Akka
  akka.ask.timeout: 180 s # default 10 s
  akka.lookup.timeout: 60 s
  akka.tcp.timeout: 80 s

  # JVM and Logging Options
  env.gc.log.opts: -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=100M
  env.jvm.error.file: true
  containerized.master.env.LD_LIBRARY_PATH: /opt/tiger/ss_lib/so:/opt/tiger/yarn_deploy/hadoop-2.6.0-cdh5.4.4/lib/native:/usr/local/hadoop/lzo/lib
  containerized.taskmanager.env.LD_LIBRARY_PATH: /opt/tiger/ss_lib/so:/opt/tiger/yarn_deploy/hadoop-2.6.0-cdh5.4.4/lib/native:/usr/local/hadoop/lzo/lib
  flink.gc.g1: false
  flink.parallel.gc.thread.use.cores: true
  flink.enable-async-logger: true

  # Env
  dc: cn

  # PyFlink on Bytedance
  base_jar: deploy/flink-1.11/basejar/flink-python-byted.jar
  bin: deploy/flink-1.11/bin/flink
  vcores: 4
  ms_url: http://ms.byted.org
  ms_zone: CN
  HADOOP_CONF_DIR: /opt/tiger/yarn_deploy/hadoop-2.6.0-cdh5.4.4/conf/
  kafka_server_url: http://kafka-config.byted.org

  # Doctor
  dashboard.data_source: bytetsd
  dtop.data_source: dtop_cn
  dtop.database: dtop
  log4j.appender.databus.channel: flink_error_log

  # Configurations for docker mode
  runtime.lib.dir: /opt/tiger/flink_deploy/deploy/flink-1.11/lib
  runtime.conf.dir: /opt/tiger/flink_deploy/deploy/flink-1.11/conf
  docker.default_image: yarn_runtime_flink:latest
  docker.image.include_lib: false
  docker.authorization: "Basic Rmxpbms6Z2huZTZrcGdqM2RvMzcxNHF0djBrZWYxbnd3aHNra2Q="
  docker.version_template_url: http://%s/api/v1/images/self-make/latest_tag/?psm=%s&region_list=%s
  docker.server: image-manager.byted.org
  docker.hub: hub.byted.org
  docker.region: China-North-LF

  # Gang scheduler
  yarn.gang-scheduler.enable: false
  yarn.gang-scheduler.jobmanager.enable: false
  yarn.gang-scheduler.node-skip-high-load: 2.5
  yarn.gang-scheduler.container-decentralized-average-weight: 10
  yarn.gang-scheduler.node-quota-usage-average-weight: 1
  yarn.gang-scheduler.wait-time-before-fatal-ms: 300000
  yarn.gang-scheduler.wait-time-before-retry-ms: 1000
  yarn.gang-scheduler.max-retry-times: 5
  yarn.gang-scheduler.downgrade-timeout-ms: 1800000

# blacklist
  blacklist.taskmanager.enabled: true
  blacklist.task.enabled: true
  blacklist.max-task-failure-num-per-host: 5
  blacklist.max-taskmanager-failure-num-per-host: 2
  blacklist.task-blacklist-max-length: 10
  blacklist.taskmanager-blacklist-max-length: 50
  blacklist.failure-timeout: "20 min"
  blacklist.check-interval: "1 min"

  # am failover recovery
  yarn.previous-container.as-pending-container: true
  resourcemanager.previous-container.as-pending-container: true
  taskmanager.release-slot-when-job-master-disconnected.enabled: true

  # Table & SQL configurations.
  # validate before execute, e.g. hive permission check.
  table.exec.validate-before-execute: true

  # configurations for hive permission check.
  table.exec.hive.permission-check.enabled: true
  table.exec.hive.permission-check.gemini-server-url: http://gemini.byted.org/api/query/default/verifyUsersPrivilege

  # enable dynamic table options by default.
  table.dynamic-table-options.enabled: true

  # Configurations for smart resources
  smart-resources.service-name: data.inf.sr_estimater.service.lf

  # Load-balance scheduler
  nmclientasync.enabled: true
  taskmanager.initial-on-start: false
  jobmanager.upload-user-jar: false
  taskmanager.number-extra-initial: 0
  taskmanager.extra-initial-fraction: 0

  # yarn slow container detection
  resourcemanager.slow-container.enabled: true
  resourcemanager.slow-container.timeout-ms: 120000
  resourcemanager.slow-container.check-interval-ms: 10000
  resourcemanager.slow-container.quantile: 0.9
  resourcemanager.slow-container.threshold-factor: 1.3
  resourcemanager.slow-container.redundant-max-factor: 0.2
  resourcemanager.slow-container.redundant-min-number: 5
  resourcemanager.slow-container.release-timeout-enabled: false
  resourcemanager.slow-container.release-timeout-ms: 300000

  # Miscellaneous Options
  stream-partitioner.default: rescale
  job.work.dir: ${hdfs.prefix}/${clusterName}/1.11/
  flink.partition-discovery.interval-millis: 600000 # default 10min for kafka partition auto discovery interval.

  # smart-resource
  taskmanager.network.memory.lazy-allocate: true

  # ipv6
  ipv6.enabled: false

  # bytedance streaming default config
  bytedance.streaming.yarn.check.application.name.unique: true
  bytedance.streaming.yarn.check.application.name.unique.region: false
  bytedance.streaming.yarn.gang-scheduler.enable: true
  bytedance.streaming.yarn.gang-scheduler.jobmanager.enable: true
  bytedance.streaming.yarn.gang-scheduler.container-descheduler.enable: false
  bytedance.streaming.yarn.gang-scheduler.container-descheduler.disk-type-enable: true
  bytedance.streaming.taskmanager.initial-on-start: true
  bytedance.streaming.taskmanager.number-initial-percentage: 1.0
  bytedance.streaming.flink.job_api: DataStream
  bytedance.streaming.jobmanager.partition.release-during-job-execution: false
  bytedance.streaming.execution.wait-running-on-detached: true
  bytedance.streaming.cluster.evenly-spread-out-slots: false
  bytedance.streaming.jobmanager.slot-sharing-execution-slot-allocator.enabled: true
  bytedance.streaming.slot-pool.round-robin: true
  bytedance.streaming.slot-pool.min-resource.simplify-enabled: true
  bytedance.streaming.yarn.res-lake.enabled: false
  bytedance.streaming.register-dashboard.enabled: true

  # hdfs conf for checkpoint
  checkpoint.hdfs.prefix: hdfs://hdfsvip/home/byte_flink_checkpoint_20210220
  flink.checkpoint.hdfs.dfs.datanode.socket.write.timeout: 30000
  flink.checkpoint.hdfs.dfs.client.socket-timeout: 30000
  flink.checkpoint.hdfs.ipc.client.ping: false
  flink.checkpoint.hdfs.ipc.ping.interval: 10000
  flink.checkpoint.hdfs.ipc.client.connect.max.retries.on.timeouts: 5
  flink.checkpoint.hdfs.ipc.client.connect.timeout: 2000
  flink.checkpoint.hdfs.ipc.client.connect.retry.interval: 500
  flink.checkpoint.hdfs.io.file.buffer.size: 1048576
  flink.checkpoint.hdfs.dfs.pipeline.fast-failover.bytes.threshold: 5242880
  flink.checkpoint.hdfs.dfs.pipeline.fast-failover.max.failover.times: 10

  # hdfs conf for slow node
  flink.hdfs.dfs.datanode.socket.write.timeout: 30000
  flink.hdfs.dfs.client.socket-timeout: 30000
  flink.hdfs.ipc.client.ping: false
  flink.hdfs.ipc.ping.interval: 10000
  flink.hdfs.ipc.client.connect.max.retries.on.timeouts: 5
  flink.hdfs.ipc.client.connect.timeout: 2000
  flink.hdfs.ipc.client.connect.retry.interval: 500

  # cloud shuffle service
  flink.cloud-shuffle-service.support: false
  flink.cloud-shuffle-service.coordinator.url: http://css-coordinator.byted.org
  flink.cloud-shuffle-service.get-css-jar-from-coordinator: false
  flink.cloud-shuffle-service.report-job-status-to-coordinator: true
  flink.cloud-shuffle-service.css.compression.codec: lz4

  # stateBackend cache layer
  state.backend.cache.enable: true
  state.backend.cache.maxHeapSize: 1g
  state.backend.cache.blockSize: 4m
  state.backend.cache.initial.size: 4m
  state.backend.cache.maxSize: 64m
  state.backend.cache.scale.enable: true
  state.backend.cache.gcCountThreshold: 40
  state.backend.cache.avgGcTimeThreshold: 3000
  state.backend.cache.maxGcTimeThreshold: 10000
  state.backend.cache.lowHeapThreshold: 0.4

  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: true

  #coredump
  containerized.master.env.COREDUMP_CTL_NEED_CORE: "false"
  containerized.taskmanager.env.COREDUMP_CTL_NEED_CORE: "false"

  # rocketmq api
  rmq.secret_key: urV6WxXdMo
  rmq.server_url.cn: https://mq.byted.org/api
  rmq.server_url.sg: https://mq.byted.org/api
  rmq.server_url.va: https://mq-us.byted.org/api
  rmq.server_url.boe: https://mq-boe.byted.org
  rmq.server_url.boei18n: https://mq-boe.byted.org

  # yaml
  configuration.flink-conf.dump-configuration-by-yaml: true

  ###  hdfs TTGW proxy address which used by checkpoint   ###
  #   checkpoint.hdfs.ttgw.lq: "[fdbd:dc03:fe:1019::1]:65212"
  #   checkpoint.hdfs.ttgw.lf: "[fdbd:dc01:fe:100e::1]:65212"
  #   checkpoint.hdfs.ttgw.hl: "[fdbd:dc02:fe:1016::1]:65212"
  #   checkpoint.hdfs.ttgw.yg: "[fdbd:dc05:fe:1007::1]:65212"
  ###########################################################

# yarn clusters
flink:
  dc: cn
  clusterName: flink
  high-availability.zookeeper.quorum: 10.17.58.36:2181,10.17.58.40:2181,10.17.58.44:2181,10.17.58.45:2181,10.17.58.78:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics
  bytedance.streaming.yarn.check.application.name.unique.region: true
  ipv6.supported.cluster: flink
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc01:fe:100e::1]:65212"

dw:
  dc: cn
  clusterName: dw
  high-availability.zookeeper.quorum: 10.11.43.39:2184,10.11.43.66:2184,10.224.152.92:2184,10.224.71.64:2184,10.224.71.66:2184
  hdfs.prefix: hdfs://haruna/flink_lf
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics
  bytedance.streaming.yarn.check.application.name.unique.region: true
  ipv6.supported.cluster: dw
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc01:fe:100e::1]:65212"

lepad:
  dc: cn
  clusterName: lepad
  high-availability.zookeeper.quorum: 10.8.32.25:2184,10.8.32.68:2184,10.8.32.74:2184,10.8.32.72:2184,10.8.39.142:2184
  hdfs.prefix: hdfs://haruna/flink_lf
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics
  bytedance.streaming.yarn.check.application.name.unique.region: true
  ipv6.supported.cluster: lepad
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc01:fe:100e::1]:65212"

larva:
  dc: cn
  clusterName: larva
  high-availability.zookeeper.quorum: 10.8.32.25:2184,10.8.32.68:2184,10.8.32.74:2184,10.8.32.72:2184,10.8.39.142:2184
  hdfs.prefix: hdfs://haruna/flink_lf
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: larva

locst:
  dc: cn
  clusterName: locst
  high-availability.zookeeper.quorum: 10.224.193.108:2181,10.224.193.109:2181,10.224.193.93:2181,10.224.193.95:2181,10.224.193.96:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  ipv6.supported.cluster: locst
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc01:fe:100e::1]:65212"

oryx:
  dc: cn
  clusterName: oryx
  high-availability.zookeeper.quorum: 10.8.32.25:2184,10.8.32.68:2184,10.8.32.74:2184,10.8.32.72:2184,10.8.39.142:2184
  hdfs.prefix: hdfs://haruna/flink_lf
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: oryx

default:
  dc: cn
  clusterName: default
  high-availability.zookeeper.quorum: 10.8.32.25:2184,10.8.32.68:2184,10.8.32.74:2184,10.8.32.72:2184,10.8.39.142:2184
  hdfs.prefix: hdfs://haruna/flink_lf
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  bytedance.streaming.yarn.check.application.name.unique.region: true
  ipv6.supported.cluster: default

leser:
  dc: cn
  clusterName: leser
  high-availability.zookeeper.quorum: 10.8.32.25:2184,10.8.32.68:2184,10.8.32.74:2184,10.8.32.72:2184,10.8.39.142:2184
  hdfs.prefix: hdfs://haruna/flink_lf
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics
  bytedance.streaming.yarn.check.application.name.unique.region: true
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: leser
  resourcemanager.slow-container.release-timeout-enabled: true
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc01:fe:100e::1]:65212"

lupin:
  dc: cn
  clusterName: lupin
  high-availability.zookeeper.quorum: fdbd:dc01:ff:101:d750:67ff:ac3f:fde2:2181,fdbd:dc01:ff:101:245a:b8a7:4f88:f0e4:2181,fdbd:dc01:ff:101:6ee2:9963:ea58:7f1b:2181,fdbd:dc01:ff:101:8679:f9e2:f954:d450:2181,fdbd:dc01:ff:101:75cd:2833:3c6f:3114:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  ipv6.enabled: true
  ipv6.supported.cluster: lupin

lobst:
  dc: cn
  clusterName: lobst
  high-availability.zookeeper.quorum: 10.8.32.25:2184,10.8.32.68:2184,10.8.32.74:2184,10.8.32.72:2184,10.8.39.142:2184
  hdfs.prefix: hdfs://haruna/flink_lf
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: lobst

lizad:
  dc: cn
  clusterName: lizad
  high-availability.zookeeper.quorum: 10.224.193.108:2181,10.224.193.109:2181,10.224.193.93:2181,10.224.193.95:2181,10.224.193.96:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  ipv6.supported.cluster: lizad

lemur:
  dc: cn
  clusterName: lemur
  high-availability.zookeeper.quorum: 10.224.193.108:2181,10.224.193.109:2181,10.224.193.93:2181,10.224.193.95:2181,10.224.193.96:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  ipv6.supported.cluster: lemur

loris:
  dc: cn
  clusterName: loris
  high-availability.zookeeper.quorum: 10.224.193.108:2181,10.224.193.109:2181,10.224.193.93:2181,10.224.193.95:2181,10.224.193.96:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  ipv6.supported.cluster: loris

llama:
  dc: cn
  clusterName: llama
  high-availability.zookeeper.quorum: 10.224.193.108:2181,10.224.193.109:2181,10.224.193.93:2181,10.224.193.95:2181,10.224.193.96:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  ipv6.supported.cluster: llama

lionn:
  dc: cn
  clusterName: lionn
  high-availability.zookeeper.quorum: 10.224.193.108:2181,10.224.193.109:2181,10.224.193.93:2181,10.224.193.95:2181,10.224.193.96:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  ipv6.supported.cluster: lionn

lynxs:
  dc: cn
  clusterName: lynxs
  high-availability.zookeeper.quorum: 10.224.193.108:2181,10.224.193.109:2181,10.224.193.93:2181,10.224.193.95:2181,10.224.193.96:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  ipv6.supported.cluster: lynxs

lambs:
  dc: cn
  clusterName: lambs
  high-availability.zookeeper.quorum: 10.224.193.108:2181,10.224.193.109:2181,10.224.193.93:2181,10.224.193.95:2181,10.224.193.96:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: lambs

brain-lf:
  dc: cn
  clusterName: brain-lf
  high-availability.zookeeper.quorum: 10.224.193.108:2181,10.224.193.109:2181,10.224.193.93:2181,10.224.193.95:2181,10.224.193.96:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: brain-lf

brain-hl:
  dc: cn
  clusterName: brain-hl
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: brain-hl

wj:
  dc: cn
  clusterName: wj
  high-availability.zookeeper.quorum: 10.8.32.25:2184,10.8.32.68:2184,10.8.32.74:2184,10.8.32.72:2184,10.8.39.142:2184
  hdfs.prefix: hdfs://haruna/flink_hl
  checkpoint.hdfs.prefix: hdfs://haruna/flink_hl
  blacklist.taskmanager.critical.error.enabled: true
  ipv6.supported.cluster: wj

hl:
  dc: cn
  clusterName: hl
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  blacklist.taskmanager.critical.error.enabled: true
  ipv6.supported.cluster: hl

hyrax:
  dc: cn
  clusterName: hyrax
  high-availability.zookeeper.quorum: 10.23.72.70:2181,10.23.73.159:2181,10.23.73.222:2181,10.23.73.69:2181,10.23.73.81:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics
  bytedance.streaming.yarn.check.application.name.unique.region: true
  blacklist.taskmanager.critical.error.enabled: true
  ipv6.supported.cluster: hyrax
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc02:fe:1016::1]:65212"

horse:
  dc: cn
  clusterName: horse
  high-availability.zookeeper.quorum: 10.226.22.110:2181,10.226.22.38:2181,10.226.22.84:2181,10.226.22.85:2181,10.226.22.90:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics
  bytedance.streaming.yarn.check.application.name.unique.region: true
  ipv6.supported.cluster: horse
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc02:fe:1016::1]:65212"

hibis:
  dc: cn
  clusterName: hibis
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: hibis

topi:
  dc: cn
  clusterName: topi
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  blacklist.taskmanager.critical.error.enabled: true
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: topi

hippo:
  dc: cn
  clusterName: hippo
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  blacklist.taskmanager.critical.error.enabled: true
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: hippo

heron:
  dc: cn
  clusterName: heron
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  blacklist.taskmanager.critical.error.enabled: true
  ipv6.supported.cluster: heron

herry:
  dc: cn
  clusterName: herry
  high-availability.zookeeper.quorum: 10.226.128.110:2181,10.226.128.107:2181,10.226.107.216:2181,10.226.128.132:2181,10.226.128.118:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  blacklist.taskmanager.critical.error.enabled: true
  ipv6.supported.cluster: herry
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc02:fe:1016::1]:65212"

hamer:
  dc: cn
  clusterName: hamer
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  blacklist.taskmanager.critical.error.enabled: true
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: hamer

hyena:
  dc: cn
  clusterName: hyena
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  blacklist.taskmanager.critical.error.enabled: true
  ipv6.supported.cluster: hyena

hound:
  dc: cn
  clusterName: hound
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  blacklist.taskmanager.critical.error.enabled: true
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: hound

hinny:
  dc: cn
  clusterName: hinny
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  execution.cancellation.timeout.enable: true
  ipv6.supported.cluster: hinny

hamst:
  dc: cn
  clusterName: hamst
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  execution.cancellation.timeout.enable: true
  ipv6.supported.cluster: hamst

hermt:
  dc: cn
  clusterName: hermt
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  execution.cancellation.timeout.enable: true
  ipv6.supported.cluster: hermt

husky:
  dc: cn
  clusterName: husky
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  execution.cancellation.timeout.enable: true
  ipv6.supported.cluster: husky

honet:
  dc: cn
  clusterName: honet
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  execution.cancellation.timeout.enable: true
  ipv6.supported.cluster: honet

hawks:
  dc: cn
  clusterName: hawks
  high-availability.zookeeper.quorum: 10.226.107.167:2181,10.226.118.239:2181,10.226.114.179:2181,10.226.114.177:2181,10.226.124.191:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  execution.cancellation.timeout.enable: true
  ipv6.supported.cluster: hawks

harpy:
  dc: cn
  clusterName: harpy
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  execution.cancellation.timeout.enable: true
  ipv6.supported.cluster: harpy

hares:
  dc: cn
  clusterName: hares
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  execution.cancellation.timeout.enable: true
  ipv6.supported.cluster: hares

hakes:
  dc: cn
  clusterName: hakes
  high-availability.zookeeper.quorum: 10.226.128.46:2181,10.226.128.80:2181,10.226.128.67:2181,10.226.128.74:2181,10.226.128.64:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  execution.cancellation.timeout.enable: true
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: hakes

queen:
  dc: cn
  clusterName: queen
  high-availability.zookeeper.quorum: 10.129.16.103:2181,10.129.19.99:2181,10.129.36.17:2181,10.129.42.156:2181,10.129.42.93:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  execution.cancellation.timeout.enable: true
  ipv6.supported.cluster: queen

quire:
  dc: cn
  clusterName: quire
  high-availability.zookeeper.quorum: 10.129.16.103:2181,10.129.19.99:2181,10.129.36.17:2181,10.129.42.156:2181,10.129.42.93:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  execution.cancellation.timeout.enable: true
  ipv6.supported.cluster: quire

quark:
  dc: cn
  clusterName: quark
  high-availability.zookeeper.quorum: 10.129.16.103:2181,10.129.19.99:2181,10.129.36.17:2181,10.129.42.156:2181,10.129.42.93:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  execution.cancellation.timeout.enable: true
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: quark

quinc:
  dc: cn
  clusterName: quinc
  high-availability.zookeeper.quorum: fdbd:dc03:ff:f2:0:248:247:88:2181,fdbd:dc03:ff:f2:0:248:238:96:2181,fdbd:dc03:ff:f2:0:248:233:110:2181,fdbd:dc03:ff:f2:0:248:251:90:2181,fdbd:dc03:ff:f2:0:248:231:88:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  execution.cancellation.timeout.enable: true
  ipv6.enabled: true
  ipv6.supported.cluster: quinc

quiet:
  dc: cn
  clusterName: quiet
  high-availability.zookeeper.quorum: fdbd:dc03:ff:2:1:227:38:118:2181,fdbd:dc03:ff:2:1:227:173:112:2181,fdbd:dc03:ff:2:1:227:173:101:2181,fdbd:dc03:ff:2:1:227:186:137:2181,fdbd:dc03:ff:2:1:227:181:113:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  execution.cancellation.timeout.enable: true
  ipv6.enabled: true
  ipv6.supported.cluster: quiet
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc03:fe:1019::1]:65212"

quetz:
  dc: cn
  clusterName: quetz
  high-availability.zookeeper.quorum: 10.129.16.103:2181,10.129.19.99:2181,10.129.36.17:2181,10.129.42.156:2181,10.129.42.93:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  execution.cancellation.timeout.enable: true
  ipv6.supported.cluster: quetz

quail:
  dc: cn
  clusterName: quail
  high-availability.zookeeper.quorum: 10.129.16.103:2181,10.129.19.99:2181,10.129.36.17:2181,10.129.42.156:2181,10.129.42.93:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  blacklist.taskmanager.critical.error.enabled: true
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: quail
  flink.cloud-shuffle-service.support: true

quoll:
  dc: cn
  clusterName: quoll
  high-availability.zookeeper.quorum: 10.129.16.103:2181,10.129.19.99:2181,10.129.36.17:2181,10.129.42.156:2181,10.129.42.93:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  blacklist.taskmanager.critical.error.enabled: true
  ipv6.supported.cluster: quoll
  flink.cloud-shuffle-service.support: true

quoka:
  dc: cn
  clusterName: quoka
  high-availability.zookeeper.quorum: 10.129.16.103:2181,10.129.19.99:2181,10.129.36.17:2181,10.129.42.156:2181,10.129.42.93:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics
  ipv6.enabled: true
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  bytedance.streaming.yarn.check.application.name.unique.region: true
  blacklist.taskmanager.critical.error.enabled: true
  ipv6.supported.cluster: quoka
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc03:fe:1019::1]:65212"

quaga:
  dc: cn
  clusterName: quaga
  high-availability.zookeeper.quorum: 10.227.165.243:2181,10.227.165.222:2181,10.227.178.242:2181,10.227.174.244:2181,10.227.187.245:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  bytedance.streaming.yarn.gang-scheduler.container-descheduler.enable: true
  ipv6.enabled: true
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  ipv6.supported.cluster: quaga

quele:
  dc: cn
  clusterName: quele
  high-availability.zookeeper.quorum: 10.227.165.243:2181,10.227.165.222:2181,10.227.178.242:2181,10.227.174.244:2181,10.227.187.245:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  bytedance.streaming.yarn.gang-scheduler.container-descheduler.enable: true
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  blacklist.taskmanager.critical.error.enabled: true
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true
  ipv6.supported.cluster: quele
  flink.cloud-shuffle-service.support: true

yakow:
  dc: cn
  clusterName: yakow
  high-availability.zookeeper.quorum: fdbd:dc05:ff:ff:1cbe:68bc:fdd8:bfe9:2181,fdbd:dc05:ff:ff:308b:d8ef:1304:521c:2181,fdbd:dc05:ff:ff:36a0:24af:475d:541e:2181,fdbd:dc05:ff:ff:39b3:fbb8:c4d1:6778:2181,fdbd:dc05:ff:ff:6743:9943:aac5:5f98:2181
  hdfs.prefix: hdfs://haruna/flink_yg/flink
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  execution.cancellation.timeout.enable: true
  ipv6.enabled: true
  ipv6.supported.cluster: yakow
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc05:fe:1007::1]:65212"

koala-yg:
  dc: cn
  clusterName: koala-yg
  high-availability.zookeeper.quorum: fdbd:dc05:ff:ff:33af:43f8:4d86:5675:2181,fdbd:dc05:ff:ff:3f33:712f:2e5c:6b15:2181,fdbd:dc05:ff:ff:8662:80af:bc6d:60a8:2181,fdbd:dc05:ff:ff:8b06:d280:734b:9722:2181,fdbd:dc05:ff:ff:f9cb:be0:a73a:41ec:2181
  hdfs.prefix: hdfs://haruna/flink_yg/flink
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  execution.cancellation.timeout.enable: true
  ipv6.enabled: true
  ipv6.supported.cluster: koala-yg
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc05:fe:1007::1]:65212"
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true

camel:
  dc: cn
  clusterName: camel
  high-availability.zookeeper.quorum: 10.148.16.172:2181,10.148.16.37:2181,10.148.16.90:2181,10.148.20.100:2181,10.148.20.98:2181
  hdfs.prefix: hdfs://haruna/flink_cr
  checkpoint.hdfs.prefix: hdfs://haruna/flink_cr
  ipv6.supported.cluster: camel

stork:
  dc: sg
  clusterName: stork
  high-availability.zookeeper.quorum: 10.105.4.10:2181,10.105.4.21:2181,10.105.4.35:2181,10.105.4.91:2181,10.105.4.254:2181
  hdfs.prefix: hdfs://harunasgee/flink_sgee
  checkpoint.hdfs.prefix: hdfs://harunasgee/flink_sgee
  dashboard.data_source: bytetsd_sgee
  dtop.data_source: dtop_alisg
  dtop.database: dtop_alisg
  smart-resources.service-name: data.inf.sr_estimater.service.alisg
  kafka_server_url: http://kafka-config-sg.byted.org
  docker.server: image-manager.byted.org
  docker.hub: aliyun-sin-hub.byted.org
  docker.region: Aliyun_SG
  save-meta.enabled: false
  table.exec.hive.permission-check.gemini-server-url: http://gemini-sg.byted.org/api/query/sg/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

shark:
  dc: sg
  clusterName: shark
  high-availability.zookeeper.quorum: 10.126.42.79:2181,10.126.42.186:2181,10.126.42.106:2181,10.126.42.225:2181,10.126.42.47:2181
  hdfs.prefix: hdfs://harunasglark/home/byte_flink_sglark
  checkpoint.hdfs.prefix:  hdfs://harunasglark/home/byte_flink_sglark_checkpoint
  dashboard.data_source: Bytetsd-Singapore-SaaS
  dtop.data_source: dtop_alisg
  dtop.database: dtop_alisg
  smart-resources.service-name: data.inf.sr_estimater.service.alisg
  kafka_server_url: http://kafka-config-sg.byted.org
  docker.server: image-manager.byted.org
  docker.hub: aliyun-sin-hub.byted.org
  docker.region: Aliyun_SG
  save-meta.enabled: false
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_sg
  table.exec.hive.permission-check.gemini-server-url: http://gemini-sg.byted.org/api/query/sg/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

snipe:
  dc: sg
  clusterName: snipe
  high-availability.zookeeper.quorum: 10.115.61.129:2181,10.115.61.130:2181,10.115.61.131:2181,10.115.61.132:2181,10.115.61.133:2181
  hdfs.prefix: hdfs://harunasg/flink_alisg
  checkpoint.hdfs.prefix: hdfs://harunasg/home/byte_flink_checkpoint_alisg
  dashboard.data_source: bytetsd_alisg
  dtop.data_source: dtop_alisg
  dtop.database: dtop_alisg
  smart-resources.service-name: data.inf.sr_estimater.service.alisg
  kafka_server_url: http://kafka-config-sg.byted.org
  docker.server: 10.8.27.231:8002
  docker.hub: aliyun-sin-hub.byted.org
  docker.region: Aliyun_SG
  jobmeta.db.name: flink_meta_sg
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_sg
  checkpoint.client-checkpoint-verification-enable: true
  yarn.provided.lib.dirs.enabled: false
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

marmt:
  dc: va
  clusterName: marmt
  high-availability.zookeeper.quorum: 10.231.131.120:2181,10.231.131.100:2181,10.231.131.112:2181,10.231.131.124:2181,10.231.131.106:2181
  hdfs.prefix: hdfs://harunavaali/flink_maliva
  checkpoint.hdfs.prefix: hdfs://harunavaali/flink_maliva
  dashboard.data_source: bytetsd_gva
  dtop.data_source: dtop_maliva
  dtop.database: dtop_maliva
  smart-resources.service-name: data.inf.sr_estimater.service.maliva
  kafka_server_url: http://kafka-config-va.byted.org
  yaop_url: http://yaop-us.bytedance.net
  docker.server: image-manager.byted.org
  docker.hub: aliyun-va-hub.byted.org
  docker.region: Aliyun_VA
  jobmeta.db.name: flink_meta_va
  grafana.domain_url: "https://grafana-us.byted.org"
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_va
  table.exec.hive.permission-check.gemini-server-url: http://gemini-maliva.byted.org/api/query/i18n/verifyUsersPrivilege
  bytedance.streaming.yarn.check.application.name.unique.region: true
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

vitas:
  dc: va
  clusterName: vitas
  high-availability.zookeeper.quorum: 10.121.72.240:2181,10.121.73.127:2181,10.121.73.228:2181,10.121.74.227:2181,10.121.75.105:2181
  hdfs.prefix: hdfs://harunavaali/flink_maliva
  checkpoint.hdfs.prefix: hdfs://harunavaali/flink_maliva
  dashboard.data_source: bytetsd_gva
  dtop.data_source: dtop_maliva
  dtop.database: dtop_maliva
  smart-resources.service-name: data.inf.sr_estimater.service.maliva
  kafka_server_url: http://kafka-config-va.byted.org
  yaop_url: http://yaop-us.bytedance.net
  docker.server: image-manager.byted.org
  docker.hub: aliyun-va-hub.byted.org
  docker.region: Aliyun_VA
  jobmeta.db.name: flink_meta_va
  grafana.domain_url: "https://grafana-us.byted.org"
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_va
  table.exec.hive.permission-check.gemini-server-url: http://gemini-maliva.byted.org/api/query/i18n/verifyUsersPrivilege
  bytedance.streaming.yarn.check.application.name.unique.region: true
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

vivid:
  dc: va
  clusterName: vivid
  high-availability.zookeeper.quorum: 10.121.76.0:2181,10.121.76.101:2181,10.121.76.27:2181,10.121.78.109:2181,10.121.79.47:2181
  hdfs.prefix: hdfs://harunavaali/flink_maliva
  checkpoint.hdfs.prefix: hdfs://harunavaali/flink_maliva
  dashboard.data_source: bytetsd_gva
  dtop.data_source: dtop_maliva
  dtop.database: dtop_maliva
  smart-resources.service-name: data.inf.sr_estimater.service.maliva
  kafka_server_url: http://kafka-config-va.byted.org
  yaop_url: http://yaop-us.bytedance.net
  docker.server: image-manager.byted.org
  docker.hub: aliyun-va-hub.byted.org
  docker.region: Aliyun_VA
  jobmeta.db.name: flink_meta_va
  grafana.domain_url: "https://grafana-us.byted.org"
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_va
  table.exec.hive.permission-check.gemini-server-url: http://gemini-maliva.byted.org/api/query/i18n/verifyUsersPrivilege
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false
  
macaw:
  dc: va
  clusterName: macaw
  high-availability.zookeeper.quorum: 10.231.131.120:2181,10.231.131.100:2181,10.231.131.112:2181,10.231.131.124:2181,10.231.131.106:2181
  hdfs.prefix: hdfs://harunavaali/flink_maliva
  checkpoint.hdfs.prefix: hdfs://harunavaali/flink_maliva
  dashboard.data_source: bytetsd_gva
  dtop.data_source: dtop_maliva
  dtop.database: dtop_maliva
  smart-resources.service-name: data.inf.sr_estimater.service.maliva
  kafka_server_url: http://kafka-config-va.byted.org
  yaop_url: http://yaop-us.bytedance.net
  docker.server: image-manager.byted.org
  docker.hub: aliyun-va-hub.byted.org
  docker.region: Aliyun_VA
  jobmeta.db.name: flink_meta_va
  grafana.domain_url: "https://grafana-us.byted.org"
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_va
  table.exec.hive.permission-check.gemini-server-url: http://gemini-maliva.byted.org/api/query/i18n/verifyUsersPrivilege
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

mouse:
  dc: va
  clusterName: mouse
  high-availability.zookeeper.quorum: 10.231.131.120:2181,10.231.131.100:2181,10.231.131.112:2181,10.231.131.124:2181,10.231.131.106:2181
  hdfs.prefix: hdfs://harunavaali/flink_maliva
  checkpoint.hdfs.prefix: hdfs://harunavaali/flink_maliva
  dashboard.data_source: bytetsd_gva
  dtop.data_source: dtop_maliva
  dtop.database: dtop_maliva
  smart-resources.service-name: data.inf.sr_estimater.service.maliva
  kafka_server_url: http://kafka-config-va.byted.org
  yaop_url: http://yaop-us.bytedance.net
  docker.server: image-manager.byted.org
  docker.hub: aliyun-va-hub.byted.org
  docker.region: Aliyun_VA
  jobmeta.db.name: flink_meta_va
  grafana.domain_url: "https://grafana-us.byted.org"
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_va
  table.exec.hive.permission-check.gemini-server-url: http://gemini-maliva.byted.org/api/query/i18n/verifyUsersPrivilege
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

mamba:
  dc: va
  clusterName: mamba
  high-availability.zookeeper.quorum: 10.231.131.120:2181,10.231.131.100:2181,10.231.131.112:2181,10.231.131.124:2181,10.231.131.106:2181
  hdfs.prefix: hdfs://harunavaali/flink_maliva
  checkpoint.hdfs.prefix: hdfs://harunavaali/flink_maliva
  dashboard.data_source: bytetsd_gva
  dtop.data_source: dtop_maliva
  dtop.database: dtop_maliva
  smart-resources.service-name: data.inf.sr_estimater.service.maliva
  kafka_server_url: http://kafka-config-va.byted.org
  yaop_url: http://yaop-us.bytedance.net
  docker.server: image-manager.byted.org
  docker.hub: aliyun-va-hub.byted.org
  docker.region: Aliyun_VA
  jobmeta.db.name: flink_meta_va
  grafana.domain_url: "https://grafana-us.byted.org"
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_va
  table.exec.hive.permission-check.gemini-server-url: http://gemini-maliva.byted.org/api/query/i18n/verifyUsersPrivilege
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

maize:
  dc: va
  clusterName: maize
  high-availability.zookeeper.quorum: 10.231.131.120:2181,10.231.131.100:2181,10.231.131.112:2181,10.231.131.124:2181,10.231.131.106:2181
  hdfs.prefix: hdfs://harunavaali/flink_maliva
  checkpoint.hdfs.prefix: hdfs://harunavaali/flink_maliva
  dashboard.data_source: bytetsd_gva
  dtop.data_source: dtop_maliva
  dtop.database: dtop_maliva
  smart-resources.service-name: data.inf.sr_estimater.service.maliva
  kafka_server_url: http://kafka-config-va.byted.org
  yaop_url: http://yaop-us.bytedance.net
  docker.server: image-manager.byted.org
  docker.hub: aliyun-va-hub.byted.org
  docker.region: Aliyun_VA
  jobmeta.db.name: flink_meta_va
  grafana.domain_url: "https://grafana-us.byted.org"
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_va
  table.exec.hive.permission-check.gemini-server-url: http://gemini-maliva.byted.org/api/query/i18n/verifyUsersPrivilege
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

momet:
  dc: va
  clusterName: momet
  high-availability.zookeeper.quorum: 10.231.131.120:2181,10.231.131.100:2181,10.231.131.112:2181,10.231.131.124:2181,10.231.131.106:2181
  hdfs.prefix: hdfs://harunavaali/flink_maliva
  checkpoint.hdfs.prefix: hdfs://harunavaali/flink_maliva
  dashboard.data_source: bytetsd_gva
  dtop.data_source: dtop_maliva
  dtop.database: dtop_maliva
  smart-resources.service-name: data.inf.sr_estimater.service.maliva
  kafka_server_url: http://kafka-config-va.byted.org
  yaop_url: http://yaop-us.bytedance.net
  docker.server: image-manager.byted.org
  docker.hub: aliyun-va-hub.byted.org
  docker.region: Aliyun_VA
  jobmeta.db.name: flink_meta_va
  grafana.domain_url: "https://grafana-us.byted.org"
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_va
  table.exec.hive.permission-check.gemini-server-url: http://gemini-maliva.byted.org/api/query/i18n/verifyUsersPrivilege
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

giraf:
  dc: va
  clusterName: giraf
  high-availability.zookeeper.quorum: 10.231.131.120:2181,10.231.131.100:2181,10.231.131.112:2181,10.231.131.124:2181,10.231.131.106:2181
  hdfs.prefix: hdfs://harunavaali/flink_maliva
  checkpoint.hdfs.prefix: hdfs://harunavaali/flink_maliva
  dashboard.data_source: bytetsd_gva
  dtop.data_source: dtop_maliva
  dtop.database: dtop_maliva
  smart-resources.service-name: data.inf.sr_estimater.service.maliva
  kafka_server_url: http://kafka-config-va.byted.org
  yaop_url: http://yaop-us.bytedance.net
  docker.server: image-manager.byted.org
  docker.hub: aliyun-va-hub.byted.org
  docker.region: Aliyun_VA
  jobmeta.db.name: flink_meta_va
  grafana.domain_url: "https://grafana-us.byted.org"
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_va
  table.exec.hive.permission-check.gemini-server-url: http://gemini-maliva.byted.org/api/query/i18n/verifyUsersPrivilege
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

grila:
  dc: i18n_gcp
  clusterName: grila
  high-availability.zookeeper.quorum: 10.99.53.218:2181,10.99.53.219:2181,10.99.53.220:2181,10.99.53.221:2181,10.99.53.222
  fs.defaultFS: hdfs://harunava
  hdfs.prefix: hdfs://harunava/home/byte_compute_i18n_gcp
  checkpoint.hdfs.prefix: hdfs://harunava/home/byte_compute_i18n_gcp
  dashboard.data_source: bytetsd_useastred
  dtop.data_source: dtop_i18n
  dtop.database: dtop_i18n
  smart-resources.service-name: data.inf.sr_estimater.service.useast2a
  kafka_server_url: http://kafka-config-gcp.byted.org
  docker.server: image-manager.byted.org
  docker.hub: useast-red-hub.byted.org
  docker.region: US-East-Red
  yaop_url: http://yaop-gcp.bytedance.net
  jobmeta.db.name: flink_meta
  grafana.domain_url: "https://grafana-i18n.byted.org"
  register-dashboard.token: "Bearer eyJrIjoiSGlWUlRpQlZWcDhsdGJ4ZTRWcGtGc1VHcG8xT2h5UkkiLCJuIjoiZmxpbmsiLCJpZCI6MX0="
  table.exec.hive.permission-check.gemini-server-url: http://gemini-gcp.byted.org/api/query/gcp/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

gavia:
  dc: i18n_gcp
  clusterName: gavia
  high-availability.zookeeper.quorum: 10.99.53.218:2181,10.99.53.219:2181,10.99.53.220:2181,10.99.53.221:2181,10.99.53.222
  fs.defaultFS: hdfs://harunava
  hdfs.prefix: hdfs://harunava/home/byte_compute_i18n_gcp
  checkpoint.hdfs.prefix: hdfs://harunava/home/byte_compute_i18n_gcp
  dashboard.data_source: bytetsd_useastred
  dtop.data_source: dtop_i18n
  dtop.database: dtop_i18n
  smart-resources.service-name: data.inf.sr_estimater.service.useast2a
  kafka_server_url: http://kafka-config-gcp.byted.org
  docker.server: image-manager.byted.org
  docker.hub: useast-red-hub.byted.org
  docker.region: US-East-Red
  yaop_url: http://yaop-gcp.bytedance.net
  jobmeta.db.name: flink_meta
  grafana.domain_url: "https://grafana-i18n.byted.org"
  register-dashboard.token: "Bearer eyJrIjoiSGlWUlRpQlZWcDhsdGJ4ZTRWcGtGc1VHcG8xT2h5UkkiLCJuIjoiZmxpbmsiLCJpZCI6MX0="
  table.exec.hive.permission-check.gemini-server-url: http://gemini-gcp.byted.org/api/query/gcp/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

gazel:
  dc: i18n_gcp
  clusterName: gazel
  high-availability.zookeeper.quorum: 10.99.53.218:2181,10.99.53.219:2181,10.99.53.220:2181,10.99.53.221:2181,10.99.53.222
  fs.defaultFS: hdfs://harunava
  hdfs.prefix: hdfs://harunava/home/byte_compute_i18n_gcp
  checkpoint.hdfs.prefix: hdfs://harunava/home/byte_compute_i18n_gcp
  dashboard.data_source: bytetsd_useastred
  dtop.data_source: dtop_i18n
  dtop.database: dtop_i18n
  smart-resources.service-name: data.inf.sr_estimater.service.useast2a
  kafka_server_url: http://kafka-config-gcp.byted.org
  docker.server: image-manager.byted.org
  docker.hub: useast-red-hub.byted.org
  docker.region: US-East-Red
  yaop_url: http://yaop-gcp.bytedance.net
  jobmeta.db.name: flink_meta
  grafana.domain_url: "https://grafana-i18n.byted.org"
  register-dashboard.token: "Bearer eyJrIjoiSGlWUlRpQlZWcDhsdGJ4ZTRWcGtGc1VHcG8xT2h5UkkiLCJuIjoiZmxpbmsiLCJpZCI6MX0="
  table.exec.hive.permission-check.gemini-server-url: http://gemini-gcp.byted.org/api/query/gcp/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

otter:
  dc: us-ttp
  clusterName: otter
  high-availability.zookeeper.quorum: 10.113.156.88:2187,10.113.156.226:2187,10.113.156.244:2187,10.113.156.4:2187,10.113.156.42:2187
  hdfs.prefix: hdfs://harunaoci/flink_oci
  checkpoint.hdfs.prefix: hdfs://harunaoci/home/byte_flink_checkpoint
  dashboard.data_source: bytetsd_us_ttp
  dtop.data_source: inf_yarn_dtop_ttp
  dtop.database: inf_dtop_yarn_ttp
  smart-resources.service-name: data.inf.sr_estimater
  kafka_server_url: https://kafka-config-tx.tiktokd.net
  log4j.sec.mark.enabled: true
  docker.server:  image-manager.tiktokd.org
  docker.hub: hub.tiktokd.org
  docker.region: US-TTP
  yaop_url: http://yaop-tx.tiktokd.org
  jobmeta.db.name: flink_meta_us_ttp
  grafana.domain_url: "https://grafana.tiktok-usts.org"
  register-dashboard.token: "Bearer eyJrIjoibzBLV1VrdkRIVkt6eW9UQlN1UmNNRXQ2MjZuOTlJTFMiLCJuIjoiYmVhcmVyIiwiaWQiOjF9"
  table.exec.hive.permission-check.gemini-server-url: http://gemini-oci.tiktokd.org/api/query/oci/verifyUsersPrivilege
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false
  yarn.provided.lib.dirs.enabled: false

octop:
  dc: us-ttp
  clusterName: octop
  high-availability.zookeeper.quorum: 10.113.156.88:2187,10.113.156.226:2187,10.113.156.244:2187,10.113.156.4:2187,10.113.156.42:2187
  hdfs.prefix: hdfs://harunaoci/flink_oci
  checkpoint.hdfs.prefix: hdfs://harunaoci/home/byte_flink_checkpoint
  dashboard.data_source: bytetsd_us_ttp
  dtop.data_source: inf_yarn_dtop_ttp
  dtop.database: inf_dtop_yarn_ttp
  smart-resources.service-name: data.inf.sr_estimater
  kafka_server_url: https://kafka-config-tx.tiktokd.net
  log4j.sec.mark.enabled: true
  docker.server:  image-manager.tiktokd.org
  docker.hub: hub.tiktokd.org
  docker.region: US-TTP
  yaop_url: http://yaop-tx.tiktokd.org
  jobmeta.db.name: flink_meta_us_ttp
  grafana.domain_url: "https://grafana.tiktok-usts.org"
  register-dashboard.token: "Bearer eyJrIjoibzBLV1VrdkRIVkt6eW9UQlN1UmNNRXQ2MjZuOTlJTFMiLCJuIjoiYmVhcmVyIiwiaWQiOjF9"
  table.exec.hive.permission-check.gemini-server-url: http://gemini-oci.tiktokd.org/api/query/oci/verifyUsersPrivilege
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false
  yarn.provided.lib.dirs.enabled: false

ostri:
  dc: us-ttp
  clusterName: ostri
  high-availability.zookeeper.quorum: 10.113.156.88:2187,10.113.156.226:2187,10.113.156.244:2187,10.113.156.4:2187,10.113.156.42:2187
  hdfs.prefix: hdfs://harunaoci/flink_oci
  checkpoint.hdfs.prefix: hdfs://harunaoci/home/byte_flink_checkpoint
  dashboard.data_source: bytetsd_us_ttp
  dtop.data_source: inf_yarn_dtop_ttp
  dtop.database: inf_dtop_yarn_ttp
  smart-resources.service-name: data.inf.sr_estimater
  kafka_server_url: https://kafka-config-tx.tiktokd.net
  log4j.sec.mark.enabled: true
  docker.server:  image-manager.tiktokd.org
  docker.hub: hub.tiktokd.org
  docker.region: US-TTP
  yaop_url: http://yaop-tx.tiktokd.org
  jobmeta.db.name: flink_meta_us_ttp
  grafana.domain_url: "https://grafana.tiktok-usts.org"
  register-dashboard.token: "Bearer eyJrIjoibzBLV1VrdkRIVkt6eW9UQlN1UmNNRXQ2MjZuOTlJTFMiLCJuIjoiYmVhcmVyIiwiaWQiOjF9"
  table.exec.hive.permission-check.gemini-server-url: http://gemini-oci.tiktokd.org/api/query/oci/verifyUsersPrivilege
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false
  yarn.provided.lib.dirs.enabled: false

alisg:
  dc: sg
  clusterName: alisg
  high-availability.zookeeper.quorum: 10.115.61.129:2181,10.115.61.130:2181,10.115.61.131:2181,10.115.61.132:2181,10.115.61.133:2181
  hdfs.prefix: hdfs://harunasg/flink_alisg
  checkpoint.hdfs.prefix: hdfs://harunasg/home/byte_flink_checkpoint_alisg
  dashboard.data_source: bytetsd_alisg
  dtop.data_source: dtop_alisg
  dtop.database: dtop_alisg
  smart-resources.service-name: data.inf.sr_estimater.service.alisg
  kafka_server_url: http://kafka-config-sg.byted.org
  docker.server: 10.8.27.231:8002
  docker.hub: aliyun-sin-hub.byted.org
  docker.region: Aliyun_SG
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_sg
  table.exec.hive.permission-check.gemini-server-url: http://gemini-sg.byted.org/api/query/sg/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

sloth:
  dc: sg
  clusterName: sloth
  high-availability.zookeeper.quorum: 10.245.24.23:2181,10.245.30.27:2181,10.245.30.47:2181,10.245.9.27:2181,10.245.9.34:2181
  hdfs.prefix: hdfs://harunasg/flink_sg_sg1
  checkpoint.hdfs.prefix: hdfs://harunasg/home/byte_flink_checkpoint_alisg
  dashboard.data_source: bytetsd_alisg
  dtop.data_source: dtop_alisg
  dtop.database: dtop_alisg
  smart-resources.service-name: data.inf.sr_estimater.service.sg1
  kafka_server_url: http://kafka-config-sg.byted.org
  docker.server: image-manager.byted.org
  docker.hub: aliyun-sin-hub.byted.org
  docker.region: Aliyun_SG
  jobmeta.db.name: flink_meta_sg
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_sg
  table.exec.hive.permission-check.gemini-server-url: http://gemini-sg.byted.org/api/query/sg/verifyUsersPrivilege
  bytedance.streaming.yarn.check.application.name.unique.region: true
  yarn.provided.lib.dirs.enabled: false
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

snail:
  dc: sg
  clusterName: snail
  high-availability.zookeeper.quorum: 10.245.24.23:2181,10.245.30.27:2181,10.245.30.47:2181,10.245.9.27:2181,10.245.9.34:2181
  hdfs.prefix: hdfs://harunasg/flink_sg_sg1
  checkpoint.hdfs.prefix: hdfs://harunasg/home/byte_flink_checkpoint_alisg
  dashboard.data_source: bytetsd_alisg
  dtop.data_source: dtop_alisg
  dtop.database: dtop_alisg
  smart-resources.service-name: data.inf.sr_estimater.service.sg1
  kafka_server_url: http://kafka-config-sg.byted.org
  docker.server: image-manager.byted.org
  docker.hub: aliyun-sin-hub.byted.org
  docker.region: Aliyun_SG
  jobmeta.db.name: flink_meta_sg
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_sg
  table.exec.hive.permission-check.gemini-server-url: http://gemini-sg.byted.org/api/query/sg/verifyUsersPrivilege
  bytedance.streaming.yarn.check.application.name.unique.region: true
  yarn.provided.lib.dirs.enabled: false
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

snake:
  dc: sg
  clusterName: snake
  high-availability.zookeeper.quorum: 10.245.24.23:2181,10.245.30.27:2181,10.245.30.47:2181,10.245.9.27:2181,10.245.9.34:2181
  hdfs.prefix: hdfs://harunasg/flink_sg_sg1
  checkpoint.hdfs.prefix: hdfs://harunasg/home/byte_flink_checkpoint_alisg
  dashboard.data_source: bytetsd_alisg
  dtop.data_source: dtop_alisg
  dtop.database: dtop_alisg
  smart-resources.service-name: data.inf.sr_estimater.service.sg1
  kafka_server_url: http://kafka-config-sg.byted.org
  docker.server: image-manager.byted.org
  docker.hub: aliyun-sin-hub.byted.org
  docker.region: Aliyun_SG
  jobmeta.db.name: flink_meta_sg
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_sg
  table.exec.hive.permission-check.gemini-server-url: http://gemini-sg.byted.org/api/query/sg/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

spider:
  dc: sg
  clusterName: spider
  high-availability.zookeeper.quorum: 10.245.24.23:2181,10.245.30.27:2181,10.245.30.47:2181,10.245.9.27:2181,10.245.9.34:2181
  hdfs.prefix: hdfs://harunasg/flink_sg_sg1
  checkpoint.hdfs.prefix: hdfs://harunasg/home/byte_flink_checkpoint_alisg
  dashboard.data_source: bytetsd_alisg
  dtop.data_source: dtop_alisg
  dtop.database: dtop_alisg
  smart-resources.service-name: data.inf.sr_estimater.service.sg1
  kafka_server_url: http://kafka-config-sg.byted.org
  docker.server: image-manager.byted.org
  docker.hub: aliyun-sin-hub.byted.org
  docker.region: Aliyun_SG
  jobmeta.db.name: flink_meta_sg
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_sg
  table.exec.hive.permission-check.gemini-server-url: http://gemini-sg.byted.org/api/query/sg/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

saola:
  dc: sg
  clusterName: saola
  high-availability.zookeeper.quorum: 10.111.64.47:2181,10.111.64.48:2181,10.111.64.49:2181,10.111.64.50:2181,10.111.64.51:2181
  hdfs.prefix: hdfs://harunasg/home/byte_flink_sg2
  checkpoint.hdfs.prefix: hdfs://harunasg/home/byte_flink_checkpoint_sg2
  dashboard.data_source: Bytetsd-Singapore-Central
  grafana.domain_url: "https://grafana-i18n.byted.org"
  register-dashboard.token: "Bearer eyJrIjoiSGlWUlRpQlZWcDhsdGJ4ZTRWcGtGc1VHcG8xT2h5UkkiLCJuIjoiZmxpbmsiLCJpZCI6MX0="
  dtop.data_source: dtop_alisg
  dtop.database: dtop_alisg
  smart-resources.service-name: data.inf.sr_estimater.service.sg1
  kafka_server_url: http://kafka-config-sg.byted.org
  docker.server: image-manager.byted.org
  docker.hub: aliyun-sin-hub.byted.org
  docker.region: Aliyun_SG
  jobmeta.db.name: flink_meta_sg
  metrics.reporter.databus_reporter.warehouse.channel: flink_dw_metrics_sg
  yarn.provided.lib.dirs.enabled: false

icing:
  dc: i18n_gcp
  clusterName: icing
  high-availability.zookeeper.quorum: 10.117.202.69:2181,10.117.202.31:2181,10.117.202.52:2181,10.117.202.63:2181,10.117.202.81:2181
  hdfs.prefix: hdfs://harunaind/home/byte_flink_ind
  checkpoint.hdfs.prefix: hdfs://harunaind/home/byte_flink_checkpoint
  grafana.domain_url: "https://grafana-i18n.byted.org"
  register-dashboard.token: "Bearer eyJrIjoiSGlWUlRpQlZWcDhsdGJ4ZTRWcGtGc1VHcG8xT2h5UkkiLCJuIjoiZmxpbmsiLCJpZCI6MX0="
  dashboard.data_source: Bytetsd-Asia-South
  dtop.data_source: dtop_alisg
  dtop.database: dtop_alisg
  kafka_server_url: http://kafka-config-sg.byted.org
  docker.server: image-manager.byted.org
  docker.hub: aliyun-sin-hub.byted.org
  docker.region: Aliyun_SG
  table.exec.hive.permission-check.gemini-server-url: http://gemini-ind.byted.org/api/query/ind/verifyUsersPrivilege
  yarn.provided.lib.dirs.enabled: false

boe:
  dc: vm
  clusterName: boe
  high-availability.zookeeper.quorum: 10.225.33.2:2181,10.225.28.3:2181,10.225.33.6:2181,10.225.125.22:2181,10.225.125.29:2181
  hdfs.prefix: hdfs://westeros/flink_boe
  checkpoint.hdfs.prefix: hdfs://westeros/flink_boe
  dtop.database: inf_yarn_dtop_boe
  dtop.data_source: dtop_boe
  dashboard.data_source: bytetsd_boe
  kafka_server_url: http://kafka-config-boe.byted.org
  log4j.appender.databus.channel: yarn_container_level_log_boe
  yaop_url: https://yaop-boe.bytedance.net
  yaop_token: 0ed7ea2fce8d4801a9d79adde7a91211
  cluster.evenly-spread-out-slots: true
  grafana.domain_url: "https://grafana-boe.byted.org"
  register-dashboard.token: "Bearer eyJrIjoiTURSV01QeWNDZXJXYUNBdEhkSU94U2tKajU2M1BVM24iLCJuIjoiZmxpbmsiLCJpZCI6MX0="
  table.exec.hive.permission-check.enabled: false
  yarn.provided.lib.dirs.enabled: false

cof:
  dc: vm
  clusterName: cof
  high-availability.zookeeper.quorum: 10.225.125.22:2181,10.225.125.29:2181,10.225.28.3:2181,10.225.33.2:2181,10.225.33.6:2181
  hdfs.prefix: hdfs://westeros/flink_cof
  checkpoint.hdfs.prefix: hdfs://westeros/flink_cof
  dashboard.data_source: bytetsd_cof
  kafka_server_url: http://kafka-config.byted.org
  log4j.appender.databus.channel: yarn_container_level_log
  yaop_url: https://yaop-boe.bytedance.net
  yaop_token: 0ed7ea2fce8d4801a9d79adde7a91211
  cluster.evenly-spread-out-slots: true
  table.exec.hive.permission-check.enabled: false
  yarn.provided.lib.dirs.enabled: false

swan:
  dc: ka
  clusterName: swan
  high-availability.zookeeper.quorum: 10.230.2.2:2181,10.230.2.10:2181,10.230.2.7:2181
  hdfs.prefix: hdfs://nestbackend/flink_swan
  checkpoint.hdfs.prefix: hdfs://nestbackend/flink_swan
  table.exec.hive.permission-check.enabled: false
  yarn.provided.lib.dirs.enabled: false
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

koala:
  dc: ka2
  clusterName: koala
  high-availability.zookeeper.quorum: 10.230.9.118:2181,10.230.9.121:2181,10.230.9.102:2181
  hdfs.prefix: hdfs://nestbackend/flink_swan
  checkpoint.hdfs.prefix: hdfs://nestbackend/flink_swan
  table.exec.hive.permission-check.enabled: false
  yarn.provided.lib.dirs.enabled: false
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

koala-lq:
  dc: cn
  clusterName: koala-lq
  high-availability.zookeeper.quorum: fdbd:dc03:ff:f2:1::13:2181,fdbd:dc03:ff:f2:1::f:2181,fdbd:dc03:ff:f2:1::2:2181,fdbd:dc03:ff:f2:1::4:2181,fdbd:dc03:ff:f2:1::12:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  execution.cancellation.timeout.enable: true
  ipv6.enabled: true
  ipv6.supported.cluster: koala-lq
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true

koala-hl:
  dc: cn
  clusterName: koala-hl
  high-availability.zookeeper.quorum: fdbd:dc02:ff:2:1:174:65:46:2181,fdbd:dc02:ff:2:1:174:65:56:2181,fdbd:dc02:ff:2:1:174:118:41:2181,fdbd:dc02:ff:2:1:174:94:65:2181,fdbd:dc02:ff:2:1:174:120:50:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  execution.cancellation.timeout.enable: true
  ipv6.enabled: true
  ipv6.supported.cluster: koala-hl
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true

koala-lf:
  dc: cn
  clusterName: koala-lf
  high-availability.zookeeper.quorum: fdbd:dc01:ff:101:b149:6a76:e7ca:8457:2181,fdbd:dc01:ff:101:7431:239f:55f1:adae:2181,fdbd:dc01:ff:101:1aa8:4797:ae42:138f:2181,fdbd:dc01:ff:101:4e13:3e17:a524:cdb4:2181,fdbd:dc01:ff:101:acde:a044:99cc:d7f4:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  flink.jobmanager.yarn.config.yarn.client.nodemanager-connect.max-wait-ms: 10000
  execution.cancellation.timeout.enable: true
  ipv6.enabled: true
  ipv6.supported.cluster: koala-lf
  bytedance.streaming.yarn.res-lake.enabled: false
  yarn.res-lake.enabled: true

boei18n:
  dc: vm
  clusterName: boei18n
  high-availability.zookeeper.quorum: 10.231.8.12:2181,10.231.8.51:2181,10.231.8.25:2181,10.231.8.21:2181,10.231.8.29:2181
  hdfs.prefix: hdfs://essos/flink_boei18n
  checkpoint.hdfs.prefix: hdfs://essos/flink_boei18n
  dashboard.data_source: bytetsd_boei18n
  kafka_server_url: http://kafka-config-boei18n.byted.org
  log4j.appender.databus.channel: yarn_container_level_log_boei18n
  yaop_url: http://yaop-boei18n.bytedance.net
  yaop_token: 0ed7ea2fce8d4801a9d79adde7a91211
  docker.hub: aliyun-va-hub.byted.org
  docker.namespace: yarn
  docker.region: Aliyun_VA
  grafana.domain_url: "https://grafana-boei18n.byted.org"
  register-dashboard.token: "Bearer eyJrIjoicndEUGRnNHpUZE9Gcm04VE5QSDdDV3JzbG8wWFZYa3IiLCJuIjoiZmxpbmsiLCJpZCI6MX0="
  table.exec.hive.permission-check.enabled: false
  yarn.provided.lib.dirs.enabled: false
  # local state quota limit
  state.backend.local-state.expected-max-size: 50g
  state.backend.local-state.actual-max-size: 60g
  state.backend.fail-exceed-quota-task.enable: false

# kubernetes clusters
Gaura-LF:
  dc: cn
  clusterName: Gaura-LF
  is_kubernetes: true
  high-availability: zookeeper
  high-availability.zookeeper.quorum: 10.203.13.5:2181,10.203.22.4:2181,10.203.9.24:2181,10.203.19.4:2181,10.203.7.6:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  # todo only add MALLOC_ARENA_MAX in k8s now, because YARN already set this env which will cause overlapping problem
  containerized.master.env.MALLOC_ARENA_MAX: 4
  containerized.taskmanager.env.MALLOC_ARENA_MAX: 4
  # service & ingress related
  kubernetes.ingress.host: "lf-gaura.byted.org"
  flink.external.jar.dependencies: "connectors/flink-connector-bmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-databus-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-doris-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-htap_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-jdbc_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-loghouse-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-metrics-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-redis-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-rocketmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-tos-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-hive-1.2.2-bd31_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-kafka-0.10_2.11-1.11-byted-SNAPSHOT.jar,formats/flink-binlog-1.11-byted-SNAPSHOT.jar,formats/flink-bytes-1.11-byted-SNAPSHOT.jar,formats/flink-json-1.11-byted-SNAPSHOT.jar,formats/flink-pb-1.11-byted-SNAPSHOT.jar,formats/flink-sequence-file-1.11-byted-SNAPSHOT.jar"
  ipv6.enabled: true
  ipv6.supported.cluster: Gaura-LF
  resourcemanager.slow-container.release-timeout-enabled: true
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc01:fe:100e::1]:65212"

Gaura-HL:
  dc: cn
  clusterName: Gaura-HL
  is_kubernetes: true
  high-availability: zookeeper
  high-availability.zookeeper.quorum: 10.226.110.167:2181,10.226.110.162:2181,10.226.119.168:2181,10.226.126.143:2181,10.226.122.156:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  # todo only add MALLOC_ARENA_MAX in k8s now, because YARN already set this env which will cause overlapping problem
  containerized.master.env.MALLOC_ARENA_MAX: 4
  containerized.taskmanager.env.MALLOC_ARENA_MAX: 4
  # service & ingress related
  kubernetes.ingress.host: "hl-gaura.byted.org"
  flink.external.jar.dependencies: "connectors/flink-connector-bmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-databus-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-doris-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-htap_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-jdbc_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-loghouse-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-metrics-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-redis-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-rocketmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-tos-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-hive-1.2.2-bd31_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-kafka-0.10_2.11-1.11-byted-SNAPSHOT.jar,formats/flink-binlog-1.11-byted-SNAPSHOT.jar,formats/flink-bytes-1.11-byted-SNAPSHOT.jar,formats/flink-json-1.11-byted-SNAPSHOT.jar,formats/flink-pb-1.11-byted-SNAPSHOT.jar,formats/flink-sequence-file-1.11-byted-SNAPSHOT.jar"
  ipv6.enabled: true
  ipv6.supported.cluster: Gaura-HL
  resourcemanager.slow-container.release-timeout-enabled: true
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc02:fe:1016::1]:65212"

Echo-HL:
  dc: cn
  clusterName: Echo-HL
  is_kubernetes: true
  high-availability: zookeeper
  high-availability.zookeeper.quorum: 10.226.110.167:2181,10.226.110.162:2181,10.226.119.168:2181,10.226.126.143:2181,10.226.122.156:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  # todo only add MALLOC_ARENA_MAX in k8s now, because YARN already set this env which will cause overlapping problem
  containerized.master.env.MALLOC_ARENA_MAX: 4
  containerized.taskmanager.env.MALLOC_ARENA_MAX: 4
  # service & ingress related
  kubernetes.ingress.host: "hl-echo.byted.org"
  flink.external.jar.dependencies: "connectors/flink-connector-bmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-databus-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-doris-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-htap_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-jdbc_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-loghouse-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-metrics-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-redis-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-rocketmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-tos-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-hive-1.2.2-bd31_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-kafka-0.10_2.11-1.11-byted-SNAPSHOT.jar,formats/flink-binlog-1.11-byted-SNAPSHOT.jar,formats/flink-bytes-1.11-byted-SNAPSHOT.jar,formats/flink-json-1.11-byted-SNAPSHOT.jar,formats/flink-pb-1.11-byted-SNAPSHOT.jar,formats/flink-sequence-file-1.11-byted-SNAPSHOT.jar"
  ipv6.enabled: true
  ipv6.supported.cluster: Echo-HL
  resourcemanager.slow-container.release-timeout-enabled: true

Foxtrot-HL:
  dc: cn
  clusterName: Foxtrot-HL
  is_kubernetes: true
  high-availability: zookeeper
  high-availability.zookeeper.quorum: fdbd:dc02:ff:2:1:226:110:167:2181,fdbd:dc02:ff:2:1:226:110:162:2181,fdbd:dc02:ff:2:1:226:119:168:2181,fdbd:dc02:ff:2:1:226:126:143:2181,fdbd:dc02:ff:2:1:226:122:156:2181
  hdfs.prefix: hdfs://haruna/flink_hl
  # todo only add MALLOC_ARENA_MAX in k8s now, because YARN already set this env which will cause overlapping problem
  containerized.master.env.MALLOC_ARENA_MAX: 4
  containerized.taskmanager.env.MALLOC_ARENA_MAX: 4
  # service & ingress related
  kubernetes.ingress.host: "hl-foxtrot.byted.org"
  flink.external.jar.dependencies: "connectors/flink-connector-bmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-databus-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-doris-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-htap_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-jdbc_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-loghouse-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-metrics-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-redis-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-rocketmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-tos-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-hive-1.2.2-bd31_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-kafka-0.10_2.11-1.11-byted-SNAPSHOT.jar,formats/flink-binlog-1.11-byted-SNAPSHOT.jar,formats/flink-bytes-1.11-byted-SNAPSHOT.jar,formats/flink-json-1.11-byted-SNAPSHOT.jar,formats/flink-pb-1.11-byted-SNAPSHOT.jar,formats/flink-sequence-file-1.11-byted-SNAPSHOT.jar"
  ipv6.enabled: true
  ipv6.supported.cluster: Foxtrot-HL
  resourcemanager.slow-container.release-timeout-enabled: true

Gaura-LQ:
  dc: cn
  clusterName: Gaura-LQ
  is_kubernetes: true
  high-availability: zookeeper
  high-availability.zookeeper.quorum: 10.248.236.172:2181,10.248.248.176:2181,10.248.234.180:2181,10.248.243.170:2181,10.248.243.148:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  # todo only add MALLOC_ARENA_MAX in k8s now, because YARN already set this env which will cause overlapping problem
  containerized.master.env.MALLOC_ARENA_MAX: 4
  containerized.taskmanager.env.MALLOC_ARENA_MAX: 4
  # service & ingress related
  kubernetes.ingress.host: "lq-gaura.byted.org"
  flink.external.jar.dependencies: "connectors/flink-connector-bmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-databus-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-doris-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-htap_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-jdbc_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-loghouse-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-metrics-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-redis-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-rocketmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-tos-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-hive-1.2.2-bd31_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-kafka-0.10_2.11-1.11-byted-SNAPSHOT.jar,formats/flink-binlog-1.11-byted-SNAPSHOT.jar,formats/flink-bytes-1.11-byted-SNAPSHOT.jar,formats/flink-json-1.11-byted-SNAPSHOT.jar,formats/flink-pb-1.11-byted-SNAPSHOT.jar,formats/flink-sequence-file-1.11-byted-SNAPSHOT.jar"
  ipv6.enabled: true
  ipv6.supported.cluster: Gaura-LQ
  resourcemanager.slow-container.release-timeout-enabled: true
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc03:fe:1019::1]:65212"

Echo-LQ:
  dc: cn
  clusterName: Echo-LQ
  is_kubernetes: true
  high-availability: zookeeper
  high-availability.zookeeper.quorum: 10.248.236.172:2181,10.248.248.176:2181,10.248.234.180:2181,10.248.243.170:2181,10.248.243.148:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  # todo only add MALLOC_ARENA_MAX in k8s now, because YARN already set this env which will cause overlapping problem
  containerized.master.env.MALLOC_ARENA_MAX: 4
  containerized.taskmanager.env.MALLOC_ARENA_MAX: 4
  # service & ingress related
  kubernetes.ingress.host: "lq-echo.byted.org"
  flink.external.jar.dependencies: "connectors/flink-connector-bmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-databus-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-doris-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-htap_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-jdbc_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-loghouse-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-metrics-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-redis-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-rocketmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-tos-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-hive-1.2.2-bd31_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-kafka-0.10_2.11-1.11-byted-SNAPSHOT.jar,formats/flink-binlog-1.11-byted-SNAPSHOT.jar,formats/flink-bytes-1.11-byted-SNAPSHOT.jar,formats/flink-json-1.11-byted-SNAPSHOT.jar,formats/flink-pb-1.11-byted-SNAPSHOT.jar,formats/flink-sequence-file-1.11-byted-SNAPSHOT.jar"
  ipv6.enabled: true
  ipv6.supported.cluster: Gaura-LQ
  resourcemanager.slow-container.release-timeout-enabled: true

Foxtrot-LQ:
  dc: cn
  clusterName: Foxtrot-LQ
  is_kubernetes: true
  high-availability: zookeeper
  high-availability.zookeeper.quorum: fdbd:dc03:ff:f2:0:248:236:172:2181,fdbd:dc03:ff:f2:0:248:248:176:2181,fdbd:dc03:ff:f2:0:248:234:180:2181,fdbd:dc03:ff:f2:0:248:243:170:2181,fdbd:dc03:ff:f2:0:248:243:148:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  # todo only add MALLOC_ARENA_MAX in k8s now, because YARN already set this env which will cause overlapping problem
  containerized.master.env.MALLOC_ARENA_MAX: 4
  containerized.taskmanager.env.MALLOC_ARENA_MAX: 4
  # service & ingress related
  kubernetes.ingress.host: "lq-foxtrot.byted.org"
  flink.external.jar.dependencies: "connectors/flink-connector-bmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-databus-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-doris-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-htap_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-jdbc_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-loghouse-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-metrics-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-redis-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-rocketmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-tos-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-hive-1.2.2-bd31_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-kafka-0.10_2.11-1.11-byted-SNAPSHOT.jar,formats/flink-binlog-1.11-byted-SNAPSHOT.jar,formats/flink-bytes-1.11-byted-SNAPSHOT.jar,formats/flink-json-1.11-byted-SNAPSHOT.jar,formats/flink-pb-1.11-byted-SNAPSHOT.jar,formats/flink-sequence-file-1.11-byted-SNAPSHOT.jar"
  ipv6.enabled: true
  ipv6.supported.cluster: Foxtrot-LQ
  resourcemanager.slow-container.release-timeout-enabled: true

Gaura-YG:
  dc: cn
  clusterName: Gaura-YG
  is_kubernetes: true
  high-availability: zookeeper
  high-availability.zookeeper.quorum: fdbd:dc05:ff:ff:5131:24b1:ffda:527e:2181,fdbd:dc05:ff:ff:57d7:193a:4895:d024:2181,fdbd:dc05:ff:ff:62a4:e284:54c:7e3b:2181,fdbd:dc05:ff:ff:9ada:93c8:a89c:1374:2181,fdbd:dc05:ff:ff:ff0f:26b5:8830:aec6:2181
  hdfs.prefix: hdfs://haruna/flink_yg/flink
  # todo only add MALLOC_ARENA_MAX in k8s now, because YARN already set this env which will cause overlapping problem
  containerized.master.env.MALLOC_ARENA_MAX: 4
  containerized.taskmanager.env.MALLOC_ARENA_MAX: 4
  # service & ingress related
  kubernetes.ingress.host: "yg-gaura.byted.org"
  flink.external.jar.dependencies: "connectors/flink-connector-bmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-databus-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-doris-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-htap_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-jdbc_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-loghouse-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-metrics-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-redis-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-rocketmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-tos-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-hive-1.2.2-bd31_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-kafka-0.10_2.11-1.11-byted-SNAPSHOT.jar,formats/flink-binlog-1.11-byted-SNAPSHOT.jar,formats/flink-bytes-1.11-byted-SNAPSHOT.jar,formats/flink-json-1.11-byted-SNAPSHOT.jar,formats/flink-pb-1.11-byted-SNAPSHOT.jar,formats/flink-sequence-file-1.11-byted-SNAPSHOT.jar"
  ipv6.enabled: true
  ipv6.supported.cluster: Gaura-YG
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc05:fe:1007::1]:65212"
  resourcemanager.slow-container.release-timeout-enabled: true

cloudnative-lf:
  dc: cn
  clusterName: cloudnative-lf
  is_kubernetes: true
  high-availability: none
  high-availability.zookeeper.quorum: 10.226.22.110:2181,10.226.22.38:2181,10.226.22.84:2181,10.226.22.85:2181,10.226.22.90:2181
  hdfs.prefix: hdfs://haruna/flink_lf
  containerized.taskmanager.env.METRICS_USE_SOCK: true
  containerized.master.env.MALLOC_ARENA_MAX: 4
  containerized.taskmanager.env.MALLOC_ARENA_MAX: 4
  metrics.reporter.opentsdb_reporter.use_domain_sock: true
  # service & ingress related
  kubernetes.ingress.host: "lf-cloudnative.byted.org"
  flink.external.jar.dependencies: "connectors/flink-connector-bmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-databus-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-doris-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-htap_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-jdbc_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-loghouse-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-metrics-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-redis-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-rocketmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-tos-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-hive-1.2.2-bd31_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-kafka-0.10_2.11-1.11-byted-SNAPSHOT.jar,formats/flink-binlog-1.11-byted-SNAPSHOT.jar,formats/flink-bytes-1.11-byted-SNAPSHOT.jar,formats/flink-json-1.11-byted-SNAPSHOT.jar,formats/flink-pb-1.11-byted-SNAPSHOT.jar,formats/flink-sequence-file-1.11-byted-SNAPSHOT.jar"
  ipv6.supported.cluster: cloudnative-lf
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc01:fe:100e::1]:65212"

cloudnative-lq:
  dc: cn
  clusterName: cloudnative-lq
  is_kubernetes: true
  high-availability: none
  high-availability.zookeeper.quorum: 10.226.22.110:2181,10.226.22.38:2181,10.226.22.84:2181,10.226.22.85:2181,10.226.22.90:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  containerized.master.env.MALLOC_ARENA_MAX: 4
  containerized.taskmanager.env.MALLOC_ARENA_MAX: 4
  # service & ingress related
  kubernetes.ingress.host: "lq-cloudnative.byted.org"
  flink.external.jar.dependencies: "connectors/flink-connector-bmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-databus-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-doris-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-htap_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-jdbc_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-loghouse-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-metrics-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-redis-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-rocketmq-1.11-byted-SNAPSHOT.jar,connectors/flink-connector-tos-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-hive-1.2.2-bd31_2.11-1.11-byted-SNAPSHOT.jar,connectors/flink-sql-connector-kafka-0.10_2.11-1.11-byted-SNAPSHOT.jar,formats/flink-binlog-1.11-byted-SNAPSHOT.jar,formats/flink-bytes-1.11-byted-SNAPSHOT.jar,formats/flink-json-1.11-byted-SNAPSHOT.jar,formats/flink-pb-1.11-byted-SNAPSHOT.jar,formats/flink-sequence-file-1.11-byted-SNAPSHOT.jar"
  ipv6.supported.cluster: cloudnative-lq
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc03:fe:1019::1]:65212"

Feins-LQ:
  dc: cn
  clusterName: Feins-LQ
  is_kubernetes: true
  high-availability: zookeeper
  high-availability.zookeeper.quorum: 10.248.236.172:2181,10.248.248.176:2181,10.248.234.180:2181,10.248.243.170:2181,10.248.243.148:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  # todo only add MALLOC_ARENA_MAX in k8s now, because YARN already set this env which will cause overlapping problem
  containerized.master.env.MALLOC_ARENA_MAX: 4
  containerized.taskmanager.env.MALLOC_ARENA_MAX: 4
  # service & ingress related
  kubernetes.ingress.host: "lq-feins.byted.org"
  ipv6.enabled: true
  ipv6.supported.cluster: Feins-LQ
  resourcemanager.slow-container.release-timeout-enabled: true

Seif-BOE:
  dc: cn
  clusterName: Seif-BOE
  is_kubernetes: true
  high-availability: zookeeper
  high-availability.zookeeper.quorum: 10.225.33.2:2181,10.225.28.3:2181,10.225.33.6:2181,10.225.125.22:2181,10.225.125.29:2181
  hdfs.prefix: hdfs://westeros/flink_boe
  checkpoint.hdfs.prefix: hdfs://westeros/flink_boe
  # todo only add MALLOC_ARENA_MAX in k8s now, because YARN already set this env which will cause overlapping problem
  dashboard.data_source: bytetsd_boe
  kafka_server_url: http://kafka-config-boe.byted.org
  log4j.appender.databus.channel: yarn_container_level_log_boe
  grafana.domain_url: "https://grafana-boe.byted.org"
  register-dashboard.token: "Bearer eyJrIjoiTURSV01QeWNDZXJXYUNBdEhkSU94U2tKajU2M1BVM24iLCJuIjoiZmxpbmsiLCJpZCI6MX0="
  table.exec.hive.permission-check.enabled: false
  containerized.master.env.MALLOC_ARENA_MAX: 4
  containerized.taskmanager.env.MALLOC_ARENA_MAX: 4
  # service & ingress related
  kubernetes.ingress.host: "boe-seif.byted.org"
  ipv6.enabled: true
  ipv6.supported.cluster: Seif-BOE

Koryn-LQ:
  dc: cn
  clusterName: Koryn-LQ
  is_kubernetes: true
  high-availability: zookeeper
  high-availability.zookeeper.quorum: 10.248.236.172:2181,10.248.248.176:2181,10.248.234.180:2181,10.248.243.170:2181,10.248.243.148:2181
  hdfs.prefix: hdfs://haruna/flink_lq
  # todo only add MALLOC_ARENA_MAX in k8s now, because YARN already set this env which will cause overlapping problem
  containerized.master.env.MALLOC_ARENA_MAX: 4
  containerized.taskmanager.env.MALLOC_ARENA_MAX: 4
  # service & ingress related
  kubernetes.ingress.host: "lq-koryn.byted.org"
  ipv6.enabled: true
  register-dashboard.enabled: true
  ipv6.supported.cluster: Koryn-LQ
  flink.checkpoint.hdfs.dfs.vip.ipPort: "[fdbd:dc03:fe:1019::1]:65212"

edge:
  dc: cn
  clusterName: edge
  is_kubernetes: true
  high-availability: none
  execution.checkpointing.enable: false
  state.checkpoints.dir: "file:///opt/tiger/flink_deploy/logs/"
